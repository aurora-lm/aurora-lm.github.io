<!DOCTYPE html><html lang="en"> <head><!-- Global Metadata --><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" type="image/svg+xml" href="https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/novasky3.jpg"><meta name="generator" content="Astro v5.1.1"><!-- Canonical URL --><link rel="canonical" href="https://novasky-ai.github.io/posts/sky-t1-7B/"><!-- Sitemap --><link rel="sitemap" href="/sitemap-index.xml"><!-- Primary Meta Tags --><title>Unlocking the Potential of Reinforcement Learning in Improving Reasoning Models</title><meta name="title" content="Unlocking the Potential of Reinforcement Learning in Improving Reasoning Models"><meta name="description" content="We are excited to release Sky-T1-7B, a SOTA open-recipe 7B model on math reasoning tasks, trained with 4-step SFT->RL->SFT->RL from the Qwen2.5-Math-7B base model. We also release Sky-T1-mini, trained with simple Reinforcement Learning (RL) applied on top of the DeepSeek-R1-Distill-Qwen-7B model, achieving close to OpenAI o1-mini performance on popular math benchmarks. We conduct a series of ablation studies on SFT data scaling, RL scaling and model’s pass@k performance after SFT and RL. We observe that the Long CoT SFT in general enhances the model’s pass@k performance while RL lifts the model’s performance at lower generation budgets (i.e., pass@1), which sometimes come at a cost of the entropy of solutions."><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://novasky-ai.github.io/posts/sky-t1-7B/"><meta property="og:title" content="Unlocking the Potential of Reinforcement Learning in Improving Reasoning Models"><meta property="og:description" content="We are excited to release Sky-T1-7B, a SOTA open-recipe 7B model on math reasoning tasks, trained with 4-step SFT->RL->SFT->RL from the Qwen2.5-Math-7B base model. We also release Sky-T1-mini, trained with simple Reinforcement Learning (RL) applied on top of the DeepSeek-R1-Distill-Qwen-7B model, achieving close to OpenAI o1-mini performance on popular math benchmarks. We conduct a series of ablation studies on SFT data scaling, RL scaling and model’s pass@k performance after SFT and RL. We observe that the Long CoT SFT in general enhances the model’s pass@k performance while RL lifts the model’s performance at lower generation budgets (i.e., pass@1), which sometimes come at a cost of the entropy of solutions."><meta property="og:image" content="https://novasky-ai.github.io/blog-placeholder-1.avif"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://novasky-ai.github.io/posts/sky-t1-7B/"><meta property="twitter:title" content="Unlocking the Potential of Reinforcement Learning in Improving Reasoning Models"><meta property="twitter:description" content="We are excited to release Sky-T1-7B, a SOTA open-recipe 7B model on math reasoning tasks, trained with 4-step SFT->RL->SFT->RL from the Qwen2.5-Math-7B base model. We also release Sky-T1-mini, trained with simple Reinforcement Learning (RL) applied on top of the DeepSeek-R1-Distill-Qwen-7B model, achieving close to OpenAI o1-mini performance on popular math benchmarks. We conduct a series of ablation studies on SFT data scaling, RL scaling and model’s pass@k performance after SFT and RL. We observe that the Long CoT SFT in general enhances the model’s pass@k performance while RL lifts the model’s performance at lower generation budgets (i.e., pass@1), which sometimes come at a cost of the entropy of solutions."><meta property="twitter:image" content="https://novasky-ai.github.io/blog-placeholder-1.avif"><!-- Preline CSS --><script type="module" src="/_astro/BaseHead.astro_astro_type_script_index_0_lang.BtT675nX.js"></script><link rel="stylesheet" href="/_astro/_page_.EmnCjKm7.css">
<style>[data-astro-image]{width:100%;height:auto;-o-object-fit:var(--fit);object-fit:var(--fit);-o-object-position:var(--pos);object-position:var(--pos);aspect-ratio:var(--w) / var(--h)}[data-astro-image=responsive]{max-width:calc(var(--w) * 1px);max-height:calc(var(--h) * 1px)}[data-astro-image=fixed]{width:calc(var(--w) * 1px);height:calc(var(--h) * 1px)}
</style><script type="module" src="/_astro/page.DTIbhfSr.js"></script>
<script>!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=Object.assign(w[p]||{},{"lib":"/~partytown/","debug":false});c[f]=(c[f]||[]).concat(["dataLayer.push"])})(window,'partytown','forward');/* Partytown 0.10.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(e){p=r.createElement(e?"script":"iframe"),t._pttab=Date.now(),e||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(e?"atomics.js?v=0.10.2":"sandbox-sw.html?"+t._pttab),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);;(e=>{e.addEventListener("astro:before-swap",e=>{let r=document.body.querySelector("iframe[src*='/~partytown/']");if(r)e.newDocument.body.append(r)})})(document);</script></head> <body class="flex flex-col min-h-screen mx-auto max-w-6xl px-4 dark:prose-invert sm:px-6 lg:px-8 dark:bg-neutral-900"> <header class="flex flex-wrap md:justify-start md:flex-nowrap z-50 w-full py-7"> <nav class="relative max-w-7xl w-full flex flex-wrap md:grid md:grid-cols-12 basis-full items-center px-4 mx-auto" aria-label="Global"> <div class="md:col-span-3"> <a class="flex-none text-xl font-semibold text-neutral-900 hover:text-neutral-600 dark:text-neutral-400 dark:hover:text-neutral-200" href="/" aria-label="Astroverse"> NovaSky </a> </div> <div class="flex items-center gap-x-2 ms-auto py-1 md:ps-6 md:order-3 md:col-span-3"> <button type="button" class="py-2 px-3 inline-flex items-center gap-x-2 text-sm font-medium rounded-xl border border-transparent text-black hover:bg-neutral-100 dark:text-white dark:hover:bg-neutral-700 transition disabled:opacity-50 disabled:pointer-events-none" onclick="window.location.href='/search/'"> <svg width="1em" height="1em" class="icon-base" data-icon="tabler:search">   <symbol id="ai:tabler:search" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 10a7 7 0 1 0 14 0a7 7 0 1 0-14 0m18 11l-6-6"/></symbol><use href="#ai:tabler:search"></use>  </svg> </button> <button type="button" class="hs-dark-mode hs-dark-mode-active:hidden inline-flex items-center gap-x-2 py-2 px-3 rounded-full text-sm text-black hover:bg-white/20" data-hs-theme-click-value="dark"> <svg width="1em" height="1em" class="icon-base" data-icon="tabler:sun-filled">   <symbol id="ai:tabler:sun-filled" viewBox="0 0 24 24"><path fill="currentColor" d="M12 19a1 1 0 0 1 .993.883L13 20v1a1 1 0 0 1-1.993.117L11 21v-1a1 1 0 0 1 1-1m6.313-2.09l.094.083l.7.7a1 1 0 0 1-1.32 1.497l-.094-.083l-.7-.7a1 1 0 0 1 1.218-1.567zm-11.306.083a1 1 0 0 1 .083 1.32l-.083.094l-.7.7a1 1 0 0 1-1.497-1.32l.083-.094l.7-.7a1 1 0 0 1 1.414 0M4 11a1 1 0 0 1 .117 1.993L4 13H3a1 1 0 0 1-.117-1.993L3 11zm17 0a1 1 0 0 1 .117 1.993L21 13h-1a1 1 0 0 1-.117-1.993L20 11zM6.213 4.81l.094.083l.7.7a1 1 0 0 1-1.32 1.497l-.094-.083l-.7-.7A1 1 0 0 1 6.11 4.74zm12.894.083a1 1 0 0 1 .083 1.32l-.083.094l-.7.7a1 1 0 0 1-1.497-1.32l.083-.094l.7-.7a1 1 0 0 1 1.414 0M12 2a1 1 0 0 1 .993.883L13 3v1a1 1 0 0 1-1.993.117L11 4V3a1 1 0 0 1 1-1m0 5a5 5 0 1 1-4.995 5.217L7 12l.005-.217A5 5 0 0 1 12 7"/></symbol><use href="#ai:tabler:sun-filled"></use>  </svg> </button> <button type="button" class="hs-dark-mode hs-dark-mode-active:inline-flex hidden items-center gap-x-2 py-2 px-3 rounded-full text-sm text-white hover:bg-white/20" data-hs-theme-click-value="light"> <svg width="1em" height="1em" class="icon-base" data-icon="tabler:moon-filled">   <symbol id="ai:tabler:moon-filled" viewBox="0 0 24 24"><path fill="currentColor" d="M12 1.992a10 10 0 1 0 9.236 13.838c.341-.82-.476-1.644-1.298-1.31a6.5 6.5 0 0 1-6.864-10.787l.077-.08c.551-.63.113-1.653-.758-1.653h-.266l-.068-.006z"/></symbol><use href="#ai:tabler:moon-filled"></use>  </svg> </button> <div class="md:hidden"> <button type="button" class="hs-collapse-toggle size-[38px] flex justify-center items-center text-sm font-semibold rounded-xl text-black hover:bg-neutral-100 disabled:opacity-50 disabled:pointer-events-none dark:text-white dark:hover:bg-neutral-700" data-hs-collapse="#navbar-collapse-with-animation" aria-controls="navbar-collapse-with-animation" aria-label="Toggle navigation"> <svg width="1em" height="1em" class="hs-collapse-open:hidden flex-shrink-0 size-4" data-icon="tabler:menu-2">   <symbol id="ai:tabler:menu-2" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"/></symbol><use href="#ai:tabler:menu-2"></use>  </svg> <svg width="1em" height="1em" class="hs-collapse-open:block hidden flex-shrink-0 size-4" data-icon="tabler:menu-order">   <symbol id="ai:tabler:menu-order" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 10h16M4 14h16M9 18l3 3l3-3M9 6l3-3l3 3"/></symbol><use href="#ai:tabler:menu-order"></use>  </svg> </button> </div> </div> <div id="navbar-collapse-with-animation" class="hs-collapse hidden overflow-hidden transition-all duration-300 basis-full grow md:block md:w-auto md:basis-auto md:order-2 md:col-span-6"> <div class="flex flex-col gap-y-4 gap-x-0 mt-5 md:flex-row md:justify-center md:items-center md:gap-y-0 md:gap-x-7 md:mt-0"> <a href="/posts/about-us/" class="px-4 py-3 font-medium text-neutral-900 hover:text-neutral-600 dark:text-neutral-400 dark:hover:text-neutral-200"> About Us </a><a href="/category/One/1/" class="px-4 py-3 font-medium text-neutral-900 hover:text-neutral-600 dark:text-neutral-400 dark:hover:text-neutral-200"> Blog Posts </a><a href="/tags/" class="px-4 py-3 font-medium text-neutral-900 hover:text-neutral-600 dark:text-neutral-400 dark:hover:text-neutral-200"> Sort by Tags </a> </div> </div> </nav> </header> <main class="flex-grow">  <main> <article class="prose mx-auto dark:prose-invert"> <div class="prose-h1 text-center"> <h1>Unlocking the Potential of Reinforcement Learning in Improving Reasoning Models</h1> </div> <div> <picture> <source srcset="https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/sky-t1-7b/teaser.png 392w, https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/sky-t1-7b/teaser.png 700w, https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/sky-t1-7b/teaser.png 980w, https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/sky-t1-7b/teaser.png 1960w" type="image/avif" sizes="(max-width: 360px) 392px, 
           (max-width: 720px) 700px, 
           (max-width: 1600px) 980px, 
           1960px">  <img src="https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/sky-t1-7b/teaser.png" srcset="https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/sky-t1-7b/teaser.png 392w, https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/sky-t1-7b/teaser.png 700w, https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/sky-t1-7b/teaser.png 980w, https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/sky-t1-7b/teaser.png 1960w" alt="performance bars" sizes="(max-width: 360px) 392px, 
           (max-width: 720px) 700px, 
           (max-width: 1600px) 980px, 
           1960px" width="4405" height="3349" loading="lazy" decoding="async" class="mx-auto w-full max-w-full rounded-lg"> </picture> </div> <div> <p><strong>Figure 1:</strong> Average accuracy of different models on four popular math reasoning tasks (i.e., AIME24, AMC23, MATH500, and OlympiadBench). Sky-T1-7B demonstrates SOTA performance among 7B models (left 4 bars) trained with &#x3C;10k distilled samples from strong teacher reasoning models and Sky-T1-mini reaches SOTA performance among all open-source 7B models, including those (5th-7th bars) trained with >100k distilled samples from strong teacher reasoning models. *For <a href="https://arxiv.org/abs/2501.04519">rstar-math</a> and <a href="https://hkust-nlp.notion.site/simplerl-reason">Qwen2.5-7B-SimpleRL</a>, since the model weights are not open source, we directly use their reported numbers.</p>
<p><strong>By: <a href="https://shiyicao.com/">Shiyi Cao</a>, <a href="https://www.linkedin.com/in/slynl/">Shu Liu</a>, <a href="https://dachengli1.github.io/">Dacheng Li</a>, <a href="https://tyler-griggs.github.io/">Tyler Griggs</a>, <a href="https://www.linkedin.com/in/kourosh-hakhamaneshi-4816a58a">Kourosh Hakhamaneshi</a>, <a href="https://sumanthrh.com/about/">Sumanth Hegde</a>, <a href="https://erictang000.github.io/">Eric Tang</a>, <a href="https://shishirpatil.github.io/">Shishir G. Patil</a>, <a href="https://people.eecs.berkeley.edu/~matei/">Matei Zaharia</a>, <a href="https://people.eecs.berkeley.edu/~jegonzal/">Joey Gonzalez</a>, <a href="https://people.eecs.berkeley.edu/~istoica/">Ion Stoica</a> — Feb 13, 2025</strong></p>
<p>We are excited to release <strong>Sky-T1-7B</strong>, a SOTA open-recipe 7B model on math reasoning tasks, trained with 4-step SFT->RL->SFT->RL from the <a href="https://huggingface.co/Qwen/Qwen2.5-Math-7B">Qwen2.5-Math-7B base model</a>. We also release <strong>Sky-T1-mini</strong>, trained with simple Reinforcement Learning (RL) applied on top of the <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B">DeepSeek-R1-Distill-Qwen-7B</a> model, achieving close to OpenAI o1-mini performance on popular math benchmarks.</p>
<p>In this blog post, we also introduce a series of RL-enhanced 7B models we trained using different recipes to develop a deeper understanding of the potential of reinforcement learning in enhancing model capabilities and its relationship with Supervised Fine-Tuning (SFT). In summary, in this blog post:</p>
<ul>
<li>We show that RL can significantly improve the reasoning scores of a small model.
<ul>
<li>We demonstrate a recipe for training Sky-T1-7B with RL and SFT from the Qwen2.5-Math-7B base model using only 5k distilled data from a strong teacher model QwQ, <strong>outperforming models trained with over 100k distilled data from a much stronger teacher model DeepSeek-R1 (e.g., OpenThinker-7B trained on 117k R1 responses)</strong>. We open-source the training recipe and its artifact, Sky-T1-7B. Notably, Sky-T1-7B also reaches similar OlympiadBench performance as DeepSeek-R1-Distill-Qwen-7B, which is trained on 800K data distilled from DeepSeek-R1.</li>
<li>Second, we show that simple RL can further enhance the current SOTA 7B reasoning model DeepSeek-R1-Distill-Qwen-7B’s capability, resulting in a new SOTA open-weights 7B reasoning model Sky-T1-mini, with close to o1-mini performance. The training takes 36 hours using 8xH100s, which is around $870 according to Lambda Cloud Pricing.</li>
</ul>
</li>
<li>We conduct a series of ablation studies on SFT data scaling, RL scaling and model’s pass@k performance after SFT and RL. <strong>We observe that the Long CoT SFT in general enhances the model’s pass@k performance while RL lifts the model’s performance at lower generation budgets (i.e., pass@1), which sometimes come at a cost of the entropy of solutions.</strong></li>
</ul>
<p>To foster community progress, we open-sourced all artifacts including the training code, training recipes, model weights, and evaluation scripts.</p>
<ul>
<li><a href="https://github.com/NovaSky-AI/SkyThought"><strong>Github</strong></a>: Code for data generation, SFT, reinforcement learning training, and evaluation.</li>
<li><a href="https://huggingface.co/collections/NovaSky-AI/sky-t1-7b-67ab281da8192c1ba3e5296c"><strong>HuggingFace</strong></a>: The Huggingface collection for model checkpoints, final model weights and datasets used for <strong>Sky-T1-7B</strong> and <strong>Sky-T1-mini</strong>.</li>
</ul>
<h2 id="sky-t1-7b--trained-with-4-step-sft-and-rl">Sky-T1-7B – Trained with 4-step SFT and RL</h2>
<p><img src="https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/sky-t1-7b/7b.jpg" alt="img">
<strong>Table 1:</strong> Benchmark performance of the intermediate models trained in the 4-step pipeline. The final model achieves accuracy improvement of +10.4% on AIME24, +33.2% on MATH500, +36.8% on AMC23, +32.1% on OlympiadBench, and +21.1% on average, compared to the base model.</p>
<h3 id="step-1-sft">Step 1: SFT</h3>
<p>We use the QwQ model to generate the distillation data since <strong>the model was trained before the release of DeepSeek R1</strong> and QwQ was the only open-weights long reasoning model at the time when we trained the model. For the data mixture, we use GPT-4o-mini to classify the difficulty of the prompts according to the AoPS standard and selected math problems of difficulty higher than Level 3, Olympiads higher than Level 8, and all AIME/AMC problems in the <a href="https://huggingface.co/datasets/AI-MO/NuminaMath-CoT">NUMINA dataset</a>. We then perform rejection sampling by only accepting the solutions that match the ground truth. In total, we curated <a href="https://huggingface.co/datasets/NovaSky-AI/Sky-T1-7B-step1-sft-5k">5K responses from QwQ</a>.
Finally, we use the 5K responses to perform SFT on the Qwen2.5-Math-7B using the <a href="https://github.com/NovaSky-AI/SkyThought/blob/main/skythought/skythought_evals/models/model_configs.yaml">Sky-T1 system prompt</a>. We trained the model for 3 epochs, using a learning rate of 1e-5, and a batch size of 96. After this stage, we get the <a href="https://huggingface.co/NovaSky-AI/Sky-T1-7B-step1">Sky-T1-7B-Step1</a> model.</p>
<h3 id="step-2-rl">Step 2: RL</h3>
<p>Next, we apply the <a href="https://github.com/PRIME-RL/PRIME">PRIME</a>’s algorithms to it. We use the <a href="https://huggingface.co/datasets/PRIME-RL/Eurus-2-RL-Data">Eurus-2-RL-Data</a> for the RL training and run it for 127 steps with a batch size of 256 (~30K data). For each prompt, we generate 4 rollouts and adopt the prompt filtering optimization proposed in PRIME that filters out the problems for which all of the 4 rollouts are correct or wrong. After this stage, we get the <a href="https://huggingface.co/NovaSky-AI/Sky-T1-7B-step2">Sky-T1-7B-Step2</a> model. This stage runs on 8xH100 for around 44 hours.</p>
<p>As suggested in <a href="https://arxiv.org/pdf/2412.19437v1">DeepSeek-V3 technical report’s</a> sec 5.1, the model trained through SFT and RL can serve as a high-quality data generator. We therefore perform another round of distillation and rejection sampling on traces generated by Sky-T1-7B-Step2 and curated <a href="https://huggingface.co/datasets/NovaSky-AI/Sky-T1-7B-step2-distill-5k">5k SFT samples</a> using the same data mixture in Step 1. We fine-tune the Qwen2.5-Math-7B with these 5k samples and obtained the Sky-T1-7B-Step2-5k-distill model, which surprisingly maintains similar or even better performance than Sky-T1-7B-Step2 across the 4 benchmarks, demonstrating extremely high data-efficiency compared to the model fine-tuned with 5k QwQ traces.</p>
<h3 id="step-3-sft-again">Step 3: SFT Again</h3>
<p>Together, with the 5K data distilled from Sky-T1-7B-Step2 in Step 2 and 5K data distilled from QwQ in Step 1, we perform another round of SFT on Qwen2.5-Math-7B base model. Similarly, we trained the model for 3 epochs, using a learning rate of 1e-5, and a batch size of 96. We then get the <a href="https://huggingface.co/NovaSky-AI/Sky-T1-7B-step3">Sky-T1-7B-step3</a> model.</p>
<h3 id="step-4-rl-again">Step 4: RL Again</h3>
<p>In this stage, to speed up the RL training, we adopt the simple <a href="https://arxiv.org/abs/2402.14740">RLOO</a> algorithm without using prompt filtering and process reward model. We use the numina_amc_aime and numina_olympiads subset of the <a href="https://huggingface.co/datasets/PRIME-RL/Eurus-2-RL-Data">Eurus-2-RL-Data</a>. We run the training for 59 steps with a batch size of 256 (~15K data). For each prompt, we generate 8 rollouts. We get <a href="https://huggingface.co/NovaSky-AI/Sky-T1-7B">Sky-T1-7B</a> as the final model.</p>
<h3 id="evaluation">Evaluation</h3>
<p>For reproductivity, we perform all the evaluation using the <a href="https://github.com/QwenLM/Qwen2.5-Math/blob/main/evaluation/sh/eval.sh">Qwen’s math evaluation suite</a>. For AIME24 and AMC 23, since they only have 30 and 40 questions respectively, we evaluate their performance by sampling 8 times for each question with a temperature of 0.6 and a top-p sampling probability of 0.95 and then compute the <a href="https://arxiv.org/pdf/2107.03374">pass@1</a> (the calculation script is also provided <a href="https://github.com/NovaSky-AI/SkyThought/tree/main/scripts/qwen_eval_bon.py">here</a>). For MATH500 and OlympiadBench, we use greedy decoding.</p>
<h3 id="results">Results</h3>
<p>We report the benchmark results for models after each stage as well as the intermediate distilled model in Table 1. We also plot the models’ pass@k curves to better understand how each SFT and RL stage impacts the model’s internal capability. For comparison, we conduct another ablation experiment which runs the RLOO directly on the Qwen2.5-Math-7B base model using the <a href="https://huggingface.co/datasets/RUC-AIBOX/STILL-3-Preview-RL-Data">STILL3</a> dataset, with 4 rollouts for each prompt. We train for 104 steps and get the final model as Sky-T1-7B-Zero.</p>
<p>As shown in Figure 2, Long CoT SFT significantly improves the model’s overall pass@k performance in both AIME24 and AMC23. In AMC, the two-stage RL primarily boosts pass@1 accuracy while reducing the diversity of solutions for k = 4 to 32. In AIME, the step4 RL further enhances overall pass@k compared to the step1 SFT and step2 RL, though its impact is less pronounced compared to Sky-T1-7B-Zero.</p>
<p><img src="https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/sky-t1-7b/sft_rl_test_time.png" alt="img">
<strong>Figure 2:</strong> Pass@K curves for models trained after each step for AIME24 and AMC23.</p>
<h2 id="sky-t1-mini--simple-rl-boosts-the-performance">Sky-T1-mini – Simple RL Boosts the Performance</h2>
<p>Throughout our development of Sky-T1-7B (which was trained before the release of DeepSeek R1’s release), we found that simple RL algorithms without a Process Reward Model (PRM) work well to enhance the model’s performance. Therefore, we also apply the simple RLOO algorithm with only the verifier reward on <a href="(https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)">DeepSeek-R1-Distill-Qwen-7B</a>, the current SOTA open-source 7B reasoning model, using the <a href="https://huggingface.co/datasets/RUC-AIBOX/STILL-3-Preview-RL-Data">STILL3</a> dataset and the numina_amc_aime and numina_olympiads subset in the <a href="https://huggingface.co/datasets/PRIME-RL/Eurus-2-RL-Data">Eurus-2-RL-Data</a> dataset. We run it for 119 steps (~28 hours) with a batch size of 256 (~30k) on 8xH100, with a cutoff length of 8k and then run it for 29 steps (~8.7 hours) with a cutoff length of 16k. The final model, Sky-T1-mini, approaches o1-mini performance across the four math benchmarks, as reported in Figure 3. <strong>While we only trained the model for a short period of time with contexts cutoff (we also didn’t carefully choose the algorithms and data mixtures), the accuracy improvement is still impressive: +4% on AIME, +5.6% on OlympiadBench and +2% on average, demonstrating the potential of RL in further enhancing model’s performance beyond distillation.</strong></p>
<h2 id="complete-results">Complete Results</h2>
<p><img src="https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/sky-t1-7b/performance_stats_avg.png" alt="img">
<strong>Figure 3:</strong> Accuracy of Sky-T1-7B and Sky-T1-mini on AIME23, AMC23, MATH500, and OlympiadBench, compared with other 7B models.</p>
<h2 id="other-observations">Other Observations</h2>
<p><img src="https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/sky-t1-7b/sft_scale_rl.png" alt="img">
<strong>Figure 4:</strong> Benchmark performance of models trained with different sizes of SFT data and those further enhanced with RL.</p>
<p><img src="https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/sky-t1-7b/passk_sft_scale.png" alt="img">
<strong>Figure 5:</strong> Pass@K curves for models trained with different sizes of SFT data and those further enhanced with RL for AMC23.</p>
<p>To evaluate the impact of scaling Long CoT SFT data sizes, we scale QwQ traces from 30k to 60k to 120k. We report the benchmark performance and AMC pass@k curves for models trained with SFT and those further enhanced with RL in Figure 4 and Figure 5 respectively. The RL training here adopts the simple RLOO algorithm, using the <a href="https://huggingface.co/datasets/RUC-AIBOX/STILL-3-Preview-RL-Data">STILL3</a> dataset, with 4 rollouts per prompt.</p>
<p>From the benchmark performance plot as shown in Figure 4, while SFT enables scaling from 30k to 60k, its effectiveness plateaus beyond this point. In contrast, models trained further with RL continue to benefit from increased data, demonstrating further improvements when scaling up to 120k. This highlights the importance of RL in effectively leveraging additional SFT training data.</p>
<p>A similar pattern emerges in pass@k evaluations as shown in Figure 5. When data scales from 30k to 60k and 120k, both SFT and RL show improvement in pass@k accuracy, with RL consistently achieving better test-time scaling across data sizes than SFT. Compared to scaling from 30k to 60k, the improvements from 60k to 120k are less pronounced for both SFT and RL.</p>
<p>This figure also shows that RL primarily enhances efficiency by improving its pass@k accuracy at lower generation budgets (i.e., for small k), effectively lifting performance without requiring excessive sampling. However, this may come at a trade-off of entropy of solutions – less gains with extensive parallel sampling.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this blog post, we show that RL can further enhance the model’s capability on either lightly- or heavily-distilled models. We further conduct the pass@k experiments to demonstrate how SFT and RL will affect the model’s pass@k performance. We observe that the Long CoT SFT in general enhances the model’s pass@k performance while RL lifts the model’s performance at lower generation budgets (i.e., pass@1), which sometimes come at a cost of the entropy of solutions.</p>
<h2 id="acknowledgement">Acknowledgement</h2>
<p>This work is done at <a href="https://sky.cs.berkeley.edu/">Berkeley Sky Computing Lab</a> with generous compute support from <a href="https://www.anyscale.com/">Anyscale</a>, <a href="https://www.databricks.com/">Databricks</a>, and <a href="https://lambdalabs.com/service/gpu-cloud?srsltid=AfmBOop5FnmEFTkavVtdZDsLWvHWNg6peXtat-OXJ9MW5GMNsk756PE5">Lambda Labs</a>.</p>
<h2 id="citation">Citation</h2>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bibtex"><code><span class="line"><span style="color:#F97583">@misc</span><span style="color:#E1E4E8">{</span><span style="color:#B392F0">sky-t1-7b</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#79B8FF">  author</span><span style="color:#E1E4E8">       = </span><span style="color:#9ECBFF">{</span><span style="color:#E1E4E8">NovaSky Team</span><span style="color:#9ECBFF">}</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#79B8FF">  title</span><span style="color:#E1E4E8">        = </span><span style="color:#9ECBFF">{</span><span style="color:#E1E4E8">Unlocking the Potential of Reinforcement Learning in Improving Reasoning Models</span><span style="color:#9ECBFF">}</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#79B8FF">  howpublished</span><span style="color:#E1E4E8"> = </span><span style="color:#9ECBFF">{</span><span style="color:#E1E4E8">https://novasky-ai.github.io/posts/sky-t1-7b</span><span style="color:#9ECBFF">}</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#79B8FF">  note</span><span style="color:#E1E4E8">         = </span><span style="color:#9ECBFF">{</span><span style="color:#E1E4E8">Accessed: 2025-02-13</span><span style="color:#9ECBFF">}</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#79B8FF">  year</span><span style="color:#E1E4E8">         = </span><span style="color:#9ECBFF">{</span><span style="color:#E1E4E8">2025</span><span style="color:#9ECBFF">}</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span></code></pre> </div> <div class="prose-a:no-underline"> <span class="mb-2 mr-2 inline-block rounded-full bg-neutral-200 px-3 py-1 text-sm hover:bg-neutral-500 dark:bg-neutral-600"> <a href="/tags/Post-Training/1/">Post-Training</a> </span><span class="mb-2 mr-2 inline-block rounded-full bg-neutral-200 px-3 py-1 text-sm hover:bg-neutral-500 dark:bg-neutral-600"> <a href="/tags/Reinforcement Learning/1/">Reinforcement Learning</a> </span><span class="mb-2 mr-2 inline-block rounded-full bg-neutral-200 px-3 py-1 text-sm hover:bg-neutral-500 dark:bg-neutral-600"> <a href="/tags/Distillation/1/">Distillation</a> </span><span class="mb-2 mr-2 inline-block rounded-full bg-neutral-200 px-3 py-1 text-sm hover:bg-neutral-500 dark:bg-neutral-600"> <a href="/tags/Reasoning/1/">Reasoning</a> </span> </div> <div class="flex justify-between"> <small>Publish on <span>2025-02-13</span>，Update on <span>2025-04-11</span></small> </div> </article> <div class="mt-4"> <div class="grid grid-cols-1 gap-4 md:grid-cols-3"> <div class="relative mb-4 overflow-hidden rounded-xl bg-white transition-transform duration-500 hover:-translate-y-1 hover:scale-105 dark:bg-neutral-900"> <a href="/posts/reduce-overthinking/"> <picture> <source srcset="https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/reduce-overthinking/blue-bird-flash.jpg 392w, https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/reduce-overthinking/blue-bird-flash.jpg 700w, https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/reduce-overthinking/blue-bird-flash.jpg 980w, https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/reduce-overthinking/blue-bird-flash.jpg 1960w" type="image/avif" sizes="(max-width: 360px) 392px, 
           (max-width: 720px) 700px, 
           (max-width: 1600px) 980px, 
           1960px">  <img src="https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/reduce-overthinking/blue-bird-flash.jpg" srcset="https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/reduce-overthinking/blue-bird-flash.jpg 392w, https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/reduce-overthinking/blue-bird-flash.jpg 700w, https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/reduce-overthinking/blue-bird-flash.jpg 980w, https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/reduce-overthinking/blue-bird-flash.jpg 1960w" alt="Blue Bird Flash" sizes="(max-width: 360px) 392px, 
           (max-width: 720px) 700px, 
           (max-width: 1600px) 980px, 
           1960px" width="1024" height="1024" loading="lazy" decoding="async" class="mx-auto w-full max-w-full rounded-lg"> </picture> <div class="absolute bottom-0 end-0 start-0 bg-gradient-to-t p-4 md:p-5"> <div class="mt-16 flex items-center gap-4 text-xs text-white"> <span>Post-Training Preference-Optimization Reasoning</span> <span>2025-01-23</span> </div> <h2 class="mt-2 text-lg font-bold text-white">Think Less, Achieve More: Cut Reasoning Costs by 50% Without Sacrificing Accuracy</h2> </div> </a> </div><div class="relative mb-4 overflow-hidden rounded-xl bg-white transition-transform duration-500 hover:-translate-y-1 hover:scale-105 dark:bg-neutral-900"> <a href="/posts/sky-t1/"> <picture> <source srcset="https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/blue-bird-wider.jpeg 392w, https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/blue-bird-wider.jpeg 700w, https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/blue-bird-wider.jpeg 980w, https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/blue-bird-wider.jpeg 1960w" type="image/avif" sizes="(max-width: 360px) 392px, 
           (max-width: 720px) 700px, 
           (max-width: 1600px) 980px, 
           1960px">  <img src="https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/blue-bird-wider.jpeg" srcset="https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/blue-bird-wider.jpeg 392w, https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/blue-bird-wider.jpeg 700w, https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/blue-bird-wider.jpeg 980w, https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/blue-bird-wider.jpeg 1960w" alt="Blue Bird" sizes="(max-width: 360px) 392px, 
           (max-width: 720px) 700px, 
           (max-width: 1600px) 980px, 
           1960px" width="1424" height="1024" loading="lazy" decoding="async" class="mx-auto w-full max-w-full rounded-lg"> </picture> <div class="absolute bottom-0 end-0 start-0 bg-gradient-to-t p-4 md:p-5"> <div class="mt-16 flex items-center gap-4 text-xs text-white"> <span>Post-Training Distillation</span> <span>2025-01-10</span> </div> <h2 class="mt-2 text-lg font-bold text-white">Sky-T1: Train your own O1 preview model within $450</h2> </div> </a> </div><div class="relative mb-4 overflow-hidden rounded-xl bg-white transition-transform duration-500 hover:-translate-y-1 hover:scale-105 dark:bg-neutral-900"> <a href="/posts/about-us/"> <picture> <source srcset="https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/novasky3.jpg 392w, https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/novasky3.jpg 700w, https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/novasky3.jpg 980w, https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/novasky3.jpg 1960w" type="image/avif" sizes="(max-width: 360px) 392px, 
           (max-width: 720px) 700px, 
           (max-width: 1600px) 980px, 
           1960px">  <img src="https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/novasky3.jpg" srcset="https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/novasky3.jpg 392w, https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/novasky3.jpg 700w, https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/novasky3.jpg 980w, https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/novasky3.jpg 1960w" alt="About" sizes="(max-width: 360px) 392px, 
           (max-width: 720px) 700px, 
           (max-width: 1600px) 980px, 
           1960px" width="2481" height="2481" loading="lazy" decoding="async" class="mx-auto w-full max-w-full rounded-lg"> </picture> <div class="absolute bottom-0 end-0 start-0 bg-gradient-to-t p-4 md:p-5"> <div class="mt-16 flex items-center gap-4 text-xs text-white"> <span>Sky Computing</span> <span>2025-01-01</span> </div> <h2 class="mt-2 text-lg font-bold text-white">About Us</h2> </div> </a> </div> </div> </div> </main>  </main> <footer class="mt-auto w-full max-w-[85rem] py-10 mx-auto"> <nav class="mx-auto w-full max-w-[85rem] px-4" aria-label="Footer"> <div class="flex flex-col items-center sm:flex-row sm:justify-between"> <div id="navbar-alignment" class="internal-links sm:order-2"> <div class="mt-2 flex flex-row gap-5 sm:mt-0 sm:flex-row sm:items-center sm:ps-5"> <a href="https://sky.cs.berkeley.edu/" class="inline-flex gap-x-2 text-sm text-neutral-500 hover:text-neutral-800 dark:text-neutral-500 dark:hover:text-neutral-200"> Sky Computing Lab @ Berkeley </a> </div> </div> <div class="mt-2 flex flex-wrap gap-2 sm:order-3 sm:mb-0 sm:gap-0"> <button type="button" class="relative inline-flex h-12 w-12 items-center gap-x-2 bg-white px-4 py-3 text-neutral-500 hover:text-neutral-800 disabled:pointer-events-none disabled:opacity-50  dark:text-neutral-500 dark:hover:text-neutral-200 dark:bg-neutral-900 dark:hover:bg-neutral-900"> <a href="https://x.com/NovaSkyAI" class="absolute inset-0 z-10" aria-label="Twitter"></a> <svg width="1em" height="1em" class="icon-base" data-icon="tabler:brand-twitter">   <symbol id="ai:tabler:brand-twitter" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M22 4.01c-1 .49-1.98.689-3 .99c-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4c0 0-4.182 7.433 4 11c-1.872 1.247-3.739 2.088-6 2c3.308 1.803 6.913 2.423 10.034 1.517c3.58-1.04 6.522-3.723 7.651-7.742a13.8 13.8 0 0 0 .497-3.753c0-.249 1.51-2.772 1.818-4.013z"/></symbol><use href="#ai:tabler:brand-twitter"></use>  </svg> </button><button type="button" class="relative inline-flex h-12 w-12 items-center gap-x-2 bg-white px-4 py-3 text-neutral-500 hover:text-neutral-800 disabled:pointer-events-none disabled:opacity-50  dark:text-neutral-500 dark:hover:text-neutral-200 dark:bg-neutral-900 dark:hover:bg-neutral-900"> <a href="https://github.com/NovaSky-AI/SkyThought" class="absolute inset-0 z-10" aria-label="GitHub"></a> <svg width="1em" height="1em" class="icon-base" data-icon="tabler:brand-github">   <symbol id="ai:tabler:brand-github" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2c2.8-.3 5.5-1.4 5.5-6a4.6 4.6 0 0 0-1.3-3.2a4.2 4.2 0 0 0-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3 0 0 0-6.2 0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2 0 0 0-.1 3.2A4.6 4.6 0 0 0 4 9.5c0 4.6 2.7 5.7 5.5 6c-.6.6-.6 1.2-.5 2V21"/></symbol><use href="#ai:tabler:brand-github"></use>  </svg> </button><button type="button" class="relative inline-flex h-12 w-12 items-center gap-x-2 bg-white px-4 py-3 text-neutral-500 hover:text-neutral-800 disabled:pointer-events-none disabled:opacity-50  dark:text-neutral-500 dark:hover:text-neutral-200 dark:bg-neutral-900 dark:hover:bg-neutral-900"> <a href="https://discord.gg/RBAjeWSA" class="absolute inset-0 z-10" aria-label="Discord"></a> <svg width="1em" height="1em" class="icon-base" data-icon="tabler:brand-discord">   <symbol id="ai:tabler:brand-discord" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M8 12a1 1 0 1 0 2 0a1 1 0 0 0-2 0m6 0a1 1 0 1 0 2 0a1 1 0 0 0-2 0"/><path d="M15.5 17c0 1 1.5 3 2 3c1.5 0 2.833-1.667 3.5-3c.667-1.667.5-5.833-1.5-11.5c-1.457-1.015-3-1.34-4.5-1.5l-.972 1.923a11.9 11.9 0 0 0-4.053 0L9 4c-1.5.16-3.043.485-4.5 1.5c-2 5.667-2.167 9.833-1.5 11.5c.667 1.333 2 3 3.5 3c.5 0 2-2 2-3"/><path d="M7 16.5c3.5 1 6.5 1 10 0"/></g></symbol><use href="#ai:tabler:brand-discord"></use>  </svg> </button> </div> <div class="mt-2 sm:order-1 sm:mb-0"> <a class="flex-none text-sm text-neutral-500 hover:text-neutral-800 dark:text-neutral-500 dark:hover:text-neutral-200" href="/" aria-label="Brand">NovaSky &copy; 2025</a> </div> </div> </nav> </footer> </body></html>