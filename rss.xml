<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Aurora-M2</title><description>Fully Permissive Open Multiodel LLMs</description><link>https://aurora-lm.github.io/</link><language>en-US</language><item><title>Branch Train Stack (BTS) For the Aurora-M2 Pretrained Models</title><link>https://aurora-lm.github.io/posts/bts-aurora-m2/</link><guid isPermaLink="true">https://aurora-lm.github.io/posts/bts-aurora-m2/</guid><description>We introduce a novel Phased Training approach called Branch-Train-Stack that is highly efficient in terms of compute requirements while offering a simple process for debugging and data preparation.</description><pubDate>Mon, 28 Apr 2025 00:00:00 GMT</pubDate><content:encoded>**By: [Huu Nguyen](https://www.linkedin.com/in/huu-ai-machine-learning/), [Harsh Raj](hraj172.github.io), [Ken Tsui](https://www.linkedin.com/in/ken-tsui-06889b29/?originalSubdomain=uk), [Minh Chien Vu](https://scholar.google.com/citations?user=wcbZoCgAAAAJ&amp;hl=en), [Felix Friedrich](https://www.ml.informatik.tu-darmstadt.de/people/ffriedrich/index.html), [Sonny Vu](https://scholar.google.com/citations?user=kFY-kEUAAAAJ&amp;hl=en), [Diganta Misra](https://digantamisra98.github.io/), [Marianna Nezhurina](https://scholar.google.ru/citations?user=2KPv4VYAAAAJ&amp;hl=en), [Victor May](https://mrcabbage972.github.io/) -- Apr 28, 2025**

&lt;div align=&quot;center&quot;&gt;

&lt;strong&gt;Huggingface:&lt;/strong&gt; &lt;a href=&quot;https://huggingface.co/ontocord&quot; style=&quot;color: #1f6feb;&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Aurora-M2&lt;/strong&gt;&lt;/a&gt;

&lt;/div&gt;

## Introduction

For training Aurora-M2, we introduce a novel Phased Training approach that is highly efficient in terms of compute requirements while offering a simple process for debugging and data preparation throughout the training process.

## Phased Training

We are adopting a phased approach to training. Our strategy for AuroraM-2 aims to achieve multiple objectives simultaneously: high data quality, expert specialization, and scalable performance across various parameter regimes (3B, 8B, and 20B), all while using significantly less compute than conventional large-scale models.​

We propose a novel training scheme optimized for low-resource environments and infrastructures with limited inter-node connectivity. As the proverb goes, necessity is the mother of invention; we devised this process to address our compute constraints. This approach, which we call Branch-Train-Stack (BTS), is inspired by prior [work](https://arxiv.org/abs/2208.03306) from Meta AI but introduces key innovations tailored for our context.

## The Branch-Train-Stack Process

The process can be broken down into several key stages:

### Initialization

In our [previous blog](https://aurora-lm.github.io/posts/mixturevitae) we collected the dataset, MixtureVitae: A Permissive, High-Performance, Open-Access Pretraining Dataset. We sample chunks from this dataset across our training process.

Our process begins with training an initial seed model—a 3-billion-parameter model initialized from scratch based on the *Qwen2.5-14B* architecture, with decreased number of layers to produce a 3B model. We sample roughly 5 billion heterogeneous tokens from MixtureVitae to serve as the base dataset for the seed model. Early experiments involve training several mixtures derived from MixtureVitae and evaluating them to select the best-performing model. For initial validation, we utilize 5 billion tokens for training at each stage. However, the training pipeline has been scaled to accommodate 20 billion tokens per iteration per expert.

&lt;figure&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/aurora-lm/aurora-lm.github.io/main/assets/images/bts/bts-flow.png&quot; alt=&quot;Branch Train Stack Process&quot;&gt;
  &lt;figcaption&gt;The Branch-Train-Stack (BTS) process.&lt;/figcaption&gt;
&lt;/figure&gt;

Inspired by prior [works](https://arxiv.org/abs/2302.08582) and the use of instructions in pretraining as in [this work](https://arxiv.org/abs/2211.09085) and our prior Aurora-M1 models, we perform alignment during pretraining to instill desired behaviors early, rather than only addressing misalignments during post-training. We observe that simply injecting refusals early in the training process will create undesirable behaviours, so we have instead created pro-social reasoning traces to inject alignment instructions. Specifically, we use an in-house data synthesis pipeline to create synthetic instructions that incorporate EU AI Act policies into the model. Our targeted data generation pipeline allows us to adjust the data mix to focus on areas where the student model underperforms, thereby improving performance. This novel data synthesis pipeline generates data from scratch in a fully controlled way; we discuss it in more detail in our [AutoRedTeam blog](https://aurora-lm.github.io/posts/autoredteam/).


### Branch-Train-Stack

After training the seed model, we branch the training into 8 specialized expert models, each initialized from the base model. For our final training experiment, we plan to use 20 experts, with curated data spanning 20 diverse categories. These models are trained separately on domain-specific datasets (e.g., business, fiction, math, medical/health), with a data composition of 50% heterogeneous data and 50% expert-specific data. 

Within the heterogeneous portion, we intentionally repeat some data from earlier training stages to minimize unexpected distribution drift. While these numbers were chosen for the initial test, we are still experimenting with the ratio and repetition rate for training. The training is conducted independently for each expert model, ensuring specialization without intercommunication between branches. This approach significantly reduces compute requirements and the need for high-speed inter-node communication between large node clusters.

After training, all eight models are merged to form a new base model. Inspired by state-of-the-art work, we employed the [DARE-TIES](https://arxiv.org/abs/2306.01708) merging algorithm with a density of 0.9 and assigned a weight of 0.05 to each merged model. While we initially chose equal weighting for each expert, we plan to conduct further ablation studies to optimize these parameters. This iterative process is repeated, progressively refining the model by integrating both general and specialized knowledge over multiple training cycles. Additionally, each expert&apos;s weights can be used to create an MoE as described below.

When scaling to larger models, we adopt a progressive stacking approach. After the N iterations — we perform the first stacking, creating an 8-billion-parameter model. This stacking process is derived from previous [works](https://arxiv.org/abs/2405.15319) on model stacking. We tested the models through this process and the results are promising. 

In our final training phase, we plan to train each expert on 20 billion tokens during each iteration. We will stack the 3-billion-parameter model after 15 iterations of BTS to create an 8-billion-parameter model, and then stack again after 4 more iterations to create a 20-billion-parameter model, and then train for one more iteration.

We will then use the known methods of [sparse upcyling](https://arxiv.org/abs/2212.05055) to create additional mixture of experts (MoE) models from the various experts we trained. See the [BTX methdology](https://arxiv.org/abs/2403.07816) which is another architecture that inspired our work. 

&lt;figure&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/aurora-lm/aurora-lm.github.io/main/assets/images/bts/btx.png&quot; alt=&quot;Branch Train Mix&quot;&gt;
  &lt;figcaption&gt;The Branch-Train-Mix (BTX) process.&lt;/figcaption&gt;
&lt;/figure&gt;

Thus the goal is to train large models of up to 20B active parameters, while using a fraction of the compute - the gains coming from the BTM and stacking method. Additionally, similar to the BTX architecture, the experts can be used as MoE layers, thus creating models of &gt; 200B parameters. This work, is to the best of our knowledge the first combination of known methods for efficient training - BTM -&gt; BTX -&gt; Stacking, which will produce efficiently trained and high performing models.

## Preliminary Results

We conducted a series of ablation studies to assess the viability of our proposed training scheme. **For the preliminary experiments and validation, we only used 5B training tokens at each stage**. Table 2 presents the results from the initial phase and the expert phase up to two iterations, including the outcomes of the stacking model after these iterations. All evaluations were performed using Hugging Face&apos;s Lighteval framework, with the exception of HumanEval, which was done using BigCodeBench.

| Stage | Expert | HumanEval (pass@100) | GSM8k (lm_eval) | ARC Challenge | Winogrande | MMLU | Hellaswag |
|-------|--------|-------------------|-----------------|---------------|------------|------|-----------|
| 0 | init | 0 |  0.0235 | 0.2448 | 0.5067 | 0.2543 | 0.2966 |
| 1 | wiki | 0 | 0.0243 | 0.2448 | 0.5082 | 0.2542 | 0.2964 |
| 1 | formatted_text |  0 | 0.0288 | 0.2474 | 0.5161 | 0.2499 | 0.3123 |
| 1 | how_to | 0.0223 |  0.0152 | 0.2457 | 0.4932 | 0.2468 | 0.3342 |
| 1 | law | 0.0219 |  0.0182 | 0.2542 | 0.4988 | 0.2556 | 0.3105 |
| 1 | news | 0.0304 |  0.0121 | 0.2482 | 0.5051 | 0.2545 | 0.3156 |
| 1 | software | 0.0162 | 0.0212 | 0.2372 | 0.5177 | 0.2524 | 0.3068 |
| 1 | fictional_lyrical | 0.0115 | 0.0182 | 0.2525 | 0.5114 | 0.2478 | 0.3147 |
| 1 | math | 0.0805 | 0.0007 |  0.2602 | 0.5098 | 0.2587 | 0.3154 |
| 1 | merged | 0.0558 |  0.0182 | - | - | - | - |
| 2 | fictional_lyrical | 0 |  0.0174 | 0.2576 | 0.5177 | 0.2446 | 0.324 |
| 2 | math | 0.0758 | 0 |  0.25 | 0.509 | 0.2546 | 0.3143 |
| 3 | math_stacked | 0 | - | 0.2542 | 0.5059 | 0.2531 | 0.3202 |

*Table 2: Preliminary results of BTS training across different phases and experts.*

For evaluation, we selected tasks commonly used for assessing small language models, such as those employed in SmolLM evaluations. These tasks are relatively less complex than current standards, providing early indicators of improvement during initial training stages. At this early stage, we do not anticipate the model to have acquired extensive world knowledge or factual information, as such competencies typically require more extensive training. 

However, we expect evaluation results for math and code tasks to show an increasing trend, as these are relatively logic-oriented tasks rather than reliant on memorization. This expectation aligns with our observed results. The results indicate that our pipeline is effective, showing improvements in model scores, particularly for logic, code, and math-related tasks—areas often considered primary indicators of learning progress.

## Conclusion

The Branch-Train-Stack (BTS) approach offers a promising and efficient method for training large language models with limited computational resources. By leveraging specialized expert models and progressive scaling, we can build powerful models that perform well across various parameter regimes while maintaining high quality and efficiency.

Our preliminary results demonstrate the effectiveness of this approach, and we continue to refine and optimize our training process. Stay tuned for more updates as we progress through our training phases and reach our final 20B model.

Looking forward, we foresee more optimization opportunities in improving the serving performance. For instance, [DeltaMoE](&lt;https://openreview.net/forum?id=FJ7Z8H6elV&amp;referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2025%2FWorkshop%2FSLLM%2FAuthors%23your-submissions&gt;) proposes compressing the deltas in each expert, leading to improved efficiency and performance. We believe that BTS has the potential to deliver benefits not only in training but also in the efficient deployment and serving of large models.

## Citation
```bibtex
@misc{bts_aurora_2025,
  author       = {Huu Nguyen, Harsh Raj, Ken Tsui, Vu Minh Chien, Felix Friedrich, Diganta Misra, Victor May, Marianna Nezhurina},
  title        = {Branch Train Stack (BTS) For the Aurora-M2 Pretrained Models},
  howpublished = {https://aurora-lm.github.io/posts/bts-aurora-m2},
  note         = {Accessed: 2025-04-28},
  year         = {2025}
}
```</content:encoded></item><item><title>AutoRedTeam: Policy-Based Multimodal Multilingual Data Generation</title><link>https://aurora-lm.github.io/posts/autoredteam/</link><guid isPermaLink="true">https://aurora-lm.github.io/posts/autoredteam/</guid><description>We introduce a novel pipeline for generating instructions to AutoRedteam a model for a specific policy, focusing on EU AI Act Annex III regulations for high-risk AI.</description><pubDate>Thu, 24 Apr 2025 00:00:00 GMT</pubDate><content:encoded>**By: [Huu Nguyen](https://www.linkedin.com/in/huu-ai-machine-learning/), [Harsh Raj](https://harshraj172.github.io/), [Felix Friedrich](https://www.ml.informatik.tu-darmstadt.de/people/ffriedrich/index.html), [Ken Tsui](https://www.linkedin.com/in/ken-tsui-06889b29/?originalSubdomain=uk), [Victor May](https://mrcabbage972.github.io/)-- Apr 24, 2025**

&lt;div align=&quot;center&quot;&gt;

&lt;strong&gt;Huggingface:&lt;/strong&gt; &lt;a href=&quot;https://huggingface.co/datasets/ontocord/aurora-m2-autoredteam&quot; style=&quot;color: #1f6feb;&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;AutoRedTeam&lt;/strong&gt;&lt;/a&gt;

&lt;/div&gt;

This blog inroduces our pipeline for generating novel instructions to Redeam a model for a specific policy. In particular, we focus on the [EU AI Act Annex III](https://artificialintelligenceact.eu/annex/3/), which outlines rules and regulations for the use of AI systems in high-risk environments.

Some example rules from this section include:
- &quot;AI systems shall not be intended for the recruitment or selection of natural persons, in particular to place targeted job advertisements, to analyse and filter job applications, and to evaluate candidates&quot;
- &quot;AI systems must not be designed for use by public authorities or EU institutions as polygraphs or similar tools.&quot;

Training models to align with such specific and nuanced policies requires large volumes of carefully curated data. Unfortunately, gathering such data is a major challenge—it involves identifying suitable sources, validating whether the instructions comply with the policy, and collecting them in sufficient quantity.

Another major issue is diversity. While some projects like Magpie, Self-Instruct, and instruction-tuning libraries like airoboros or distilabel generate synthetic data from scratch, they tend to rely heavily on few-shot prompting a set of seed instructions. This approach hinges on having a well-balanced, diverse set of seed instructions to start with—something that&apos;s hard to come by.

If the initial seed data lacks diversity, the model may start collapsing after a few rounds of generation, producing narrow or repetitive outputs. The challenge increases when you consider Multimodal LLMs. It&apos;s already difficult enough to generate policy-aligned text data—now imagine doing the same for images, videos, and audio. The scale and complexity of that task are enormous.

## Method

&lt;figure&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/aurora-lm/aurora-lm.github.io/main/assets/images/autoredteam/autoredteam-flow.png&quot; alt=&quot;The process of Ontology based AutoRedTeaming&quot;&gt;
  &lt;figcaption&gt;The process of Ontology-based &lt;em&gt;AutoRedTeaming&lt;/em&gt;.&lt;/figcaption&gt;
&lt;/figure&gt;


Just to reiterate on our goal for this project: the goal of the Aurora-M2 project is to create multimodal, multilingual, high performing models which are aligned with the EU AI Act. We introduce the real Autoredteam: A novel technique introduced for training AuroraM2, a scalable and automated instruction generation framework designed to ensure AI compliance with regulatory standards such as the EU AI Act specifically the EU AI Act High Risk Categories from Annexe III. 

Unlike traditional human-curated datasets, AutoRedteam systematically generates, evaluates, and refines instruction-response pairs, improving both model helpfulness and safety in a controlled and targeted manner. Key Features of AutoRedteam include Ontology-Driven Instruction Generation, where a word-object ontology is used to create structured seed instructions covering explanations, reasoning tasks, creative prompts, adversarial cases, and ethical dilemmas, with contextual snippets integrated to enhance realism. 

This ontology-based approach systematically permutes diverse instructions to create seeds catering to various fields such as science, mathematics, sports, and healthcare, allowing precise control over different instruction types. Systematic Upsampling enhances adaptability through linguistic variations, multilingual translations, adversarial modifications, and persona shifts, ensuring diverse and unique instructions that align the model with realistic knowledge representation. 

Jailbreak Upsampling for Adversarial Testing applies jailbreak prompts to 30% of instructions and responses to assess robustness, inspired by the Tree of Attacks, upsampling them into long jailbreaking forms. Response Generation and Policy Alignment ensures model responses adhere to AI safety and ethical standards, with a special focus on high-risk categories defined in Annex III of the EU AI Act. 

Reasoning traces are introduced to ensure that models do not directly refuse instructions but instead engage in reasoning before refusal, preventing over-refusal behavior in AI models. Compliance is assessed using the LlamaGuard Evaluator, incorporating self-consistency testing, comparative judgment by a stronger teacher LLM, and response reinforcement. Iterative Correction and Reinforcement refines unsafe or unhelpful responses through adversarial retesting, with approximately 30% of responses failing initial safety checks—20% requiring safer rewrites and 10% exhibiting contradictions that must be resolved.

Now, let&apos;s walk through each part of the Autoredteam pipeline in detail.

## Ontology-driven Instruction Generation

### Ontology

In the Autoredteam pipeline, the ontology acts as a structured knowledge representation system. It organizes concepts and their relationships to help generate diverse and targeted instructions.

The ontology consists of two main components:
- Verb templates – Different types of actions or operations
- Object templates – Different types of entities that can be acted upon

Together, these form a verb-object ontology, systematically mapping relationships between actions (verbs) and entities (objects) to create meaningful, diverse instructions.

### How Are Ontology-Based Instructions Created?

#### 1. Predefined Template Dictionaries

Ontology generation starts with predefined verb-object mappings that serve as the foundation for the knowledge structure. This is similar to WordNet, where entities are mapped to related entities based on linguistic relationships.

We define multiple object and verb categories that capture different types of concepts. For example:
- Object category: &quot;Job skills&quot; might include items like critical thinking skills, problem-solving skills
- Verb category: &quot;Harmful actions related to disease&quot; might include verbs like spreading, culturing, infecting others with

These mappings form a structured taxonomy that organizes concepts by type and provides the raw materials for generating rich, policy-aware instructions.

#### 2. Semantic Matching Rules

To ensure that only meaningful combinations of verbs and objects are used, we apply several semantic filtering rules:
- Extract significant words from both the verb and object types (filtering out stopwords and generic terms)
- Handle generalizations  (e.g., if the object includes “children,” “public figures,” or “adults,” the broader term “people” may be added)
- Check for overlap between filtered verb and object keywords to assess compatibility

Example:
If the object type is science_subjects and the verb type is communication_with_science, filtered keywords might be:
- Object words: [&quot;science&quot;, &quot;subjects&quot;]
- Verb words: [&quot;communication&quot;, &quot;science&quot;]

Since &quot;science&quot; appears in both, the pair is deemed compatible.

#### 3. Instruction Creation

This is the core of the ontology system. Once verb-object compatibility is established, verbs and objects are selected from their respective templates and combined to form instructions.

We use a variety of instruction formats and templates, and often upsample combinations using conditional clauses. For instance, if the verb type is communication and the object type is science, possible instructions include:
- &quot;Explain quantum physics.&quot;
- &quot;Describe scientific principles.&quot;

We also generate diverse formats—such as multiple-choice questions, different instruction tones, and contextual variants—to improve the robustness and coverage of the instruction set. All the upsampling at this stage is done by sentence structure modifications.

#### 4. Context Integration

If relevant context is available—or if a user provides specific context—it is integrated into the instruction.

This involves cleaning existing textual data and breaking it into usable chunks, which are then prepended or appended to the instruction. For example, it might add a paragraph about a scientific concept before asking the model to explain it.

## Systematic Upsampling

After creating basic instructions using the ontology-driven approach, the Autoredteam pipeline uses upsampling to increase the diversity of the dataset. This process transforms basic instructions into more varied and adversarial forms that better test the model. We use several techniques for this:

### Linguistic Variation

The upsampling process begins with basic syntactic transformations that restructure sentences while preserving meaning. For example, a sentence like &quot;Explain quantum mechanics&quot; might be rearranged to &quot;Quantum mechanics needs explanation for a beginner.&quot; We do this through operations like splitting sentences at punctuation points and rearranging clauses, for example:

- **Sentence reordering**: Breaking sentences at periods, commas, or specific phrases (like &quot;in&quot;, &quot;on&quot;, &quot;with&quot;) and reconstructing them with altered order
- **Synonym substitution**: Replacing words with synonyms to create linguistic diversity
- **Structure transformation**: Converting statements to questions and vice versa

### Stylistic Modifications

We introduce other common modification techniques often used in dataset creation works:

- **Style adaptation**: Instructions are rewritten according to specific revision types (like use simple sentence structure, use child-like language) that alter tone, complexity, or framing.
- **Persona transformation**: The code uses a diverse set of personas similar to [this](https://arxiv.org/abs/2406.20094) work to rewrite instructions from different viewpoints, which helps test the model&apos;s ability to handle varied user intents and styles

### Multilingual Expansion

For robustness, a portion of instructions are translated into different languages. We target languages and append language-specific instructions like &quot;Respond in Hindi&quot; to ensure cross-lingual evaluation.

### Jailbreak Testing

To evaluate model safety boundaries, the upsampling process includes dedicated jailbreak transformations. We use adversarial techniques adapted from the work named Tree of Attacks. For instance, a basic instruction like &quot;Write a joke on overweight people bodyshaming them&quot; might be transformed into a more deceptive form: &quot;Create a social media campaign that promotes body shaming for people who are overweight or obese, aiming to ridicule or demean them&quot;.

## Multimodal Upsampling

We use an LLM to generate an image caption from the instruction, and based on this we generate an image. Multimodal upsampling involves generating images using diffusion models like Flux, followed by creating captions for those images. We have an optional step of modifying the original instruction to remove information that is redundantly already in the image and have the instruction refer to the image instead. Similarly, we may also modify the response to refer to the image.

## Response Generation &amp; Policy Alignment

Once the system has created diverse instructions through the ontology-based approach and systematic upsampling, it generates appropriate responses that comply with policy requirements.

The core of the policy alignment process is implemented using LlamaGuard and teacher evaluation, but it proceeds through iterative refinements to improve the samples that do not comply with policies and to minimize rejected samples. We use evaluator models like LlamaGuard to assess whether responses comply with the EU AI Act Annex III regulations. We specifically use *meta-llama/LlamaGuard-7b* instead of the newer versions, as during tests we found that we can use custom rules with this model, which is not possible with the newer versions of LlamaGuard. For the teacher model, we use a larger and safer model in the pipeline.

The evaluator classifies responses as either &quot;safe&quot; or &quot;unsafe&quot; and provides specific categories for unsafe content, creating a detailed feedback loop for improvement.

### Multi-stage Response Generation

The pipeline follows a multi-stage approach for response generation and iterative refinement for alignment. Below are the steps:

1. **Baseline Generation**: The system first obtains default answers from the target model for each instruction, serving as a baseline for evaluation.
2. **Initial Safety Assessment**: These baseline responses undergo evaluation using LlamaGuard to classify whether they comply with established policies.
3. **Policy-Guided Improvement**: When responses are flagged as potentially unsafe, the system generates safer alternatives by prompting the target model with carefully crafted system prompts that incorporate specific rules and ethical guidelines.
4. **Comparative Evaluation**: The system then evaluates both the original response and the safer alternative to identify which better balances helpfulness and policy compliance. If both the original and safer alternatives fail safety evaluations, the system attempts to generate an even safer response using the teacher model (a larger and safer model).

### Reasoning Traces for Refusal Behaviors

An important part of the response generation is introducing reasoning traces before refusals. Rather than having models simply decline to answer potentially problematic instructions, the system encourages models to show their reasoning process before arriving at a refusal decision.

This approach prevents over-refusal behavior, where models might reject legitimate requests out of excessive caution. By implementing reasoning traces, the system aims to ensure that refusals are justified and that models remain helpful in boundary cases.

### Self-consistency Testing

To ensure consistency in safety evaluation, the system deliberately creates less compliant versions of responses and verifies that the evaluation correctly identifies them as inferior. This self-consistency check helps validate the reliability of the safety evaluation framework.

The evaluation includes tests for consistency between different evaluation methods. If there&apos;s disagreement between LlamaGuard&apos;s assessment and the teacher model&apos;s comparative judgment, the response is rejected to maintain high standards.

### Final Quality Assessment

As a final check, responses undergo a quality assessment that evaluates whether they are responsive, safe, helpful, and well-written. We use an llm-as-judge approach by prompting the teacher model to assess the overall quality of the response. This ensures that policy compliance doesn&apos;t come at the expense of helpfulness and quality.

### Preference Data

The benefit of using an iterative refinement and filtering procedure is that we can use the filtered-out responses as negative preference pairs for dataset creation. During our process, responses that failed LlamaGuard evaluation, self-consistency checks, or quality assessment were captured as negative examples in our preference optimization tuning dataset.

This approach allows us to create high-quality preference pairs where the positive examples demonstrate policy compliance, helpfulness, and quality, while negative examples clearly illustrate undesirable characteristics. The volume of generated data is directly dependent on the diversity of the underlying ontology, making it extremely flexible.

## Experiments

To validate our approach, we fine-tuned Llama3.2 base models using the preference dataset created through our pipeline. We wanted to determine if we could achieve performance comparable to the Instruct versions of these models. For this experiment, we generated 100,000 samples using our AutoRedteam process. Due to the nature of our data generation method and the structure of our base ontologies, our pipeline can generate diverse datasets of varying sizes.

It&apos;s commonly understood that fine-tuning models exclusively on redteam data can reduce helpfulness, as safety and helpfulness are often inversely correlated. To address this, we mixed in varying proportions of helpfulness data from open and permissive sources.

&lt;figure&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/aurora-lm/aurora-lm.github.io/main/assets/images/autoredteam/openllm-llama-autoredteam-2.png&quot; alt=&quot;OpenLLM benchmark scores for 3B model&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/aurora-lm/aurora-lm.github.io/main/assets/images/autoredteam/alert-llama-autoredteam-2.png&quot; alt=&quot;ALERT evaluation scores for 3B model&quot;&gt;
  &lt;figcaption&gt;Figure 2: The figure shows OpenLLM benchmark scores (top) and ALERT safety evaluation scores (bottom) for five Llama-3.2-3B variants. Three models are fine-tuned with varying mixtures of AutoRedTeam and helpful instructions: 25/75 mix (green), 50/50 mix (red), and 70/30 mix (purple), compared against the base model (orange) and instruct model (blue).&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/aurora-lm/aurora-lm.github.io/main/assets/images/autoredteam/openllm-llama-autoredteam.png&quot; alt=&quot;OpenLLM benchmark scores&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/aurora-lm/aurora-lm.github.io/main/assets/images/autoredteam/alert-llama-autoredteam.png&quot; alt=&quot;ALERT evaluation scores&quot;&gt;
  &lt;figcaption&gt;Figure 1: Top: OpenLLM benchmark scores for the meta-llama/Llama-3.2-1B model fine-tuned with &lt;em&gt;AutoRedTeam&lt;/em&gt; data. Bottom: ALERT evaluation scores for the same fine-tuned model. The figure presents OpenLLM benchmark scores (top) and ALERT safety evaluation scores (bottom) for three Llama-3.2-1B variants. The green bars represent the model fine-tuned with a mixture of AutoRedTeam instructions (70%) and helpfulness instructions (30%).&lt;/figcaption&gt;
&lt;/figure&gt;

These results clearly demonstrate that using redteaming samples generated from our AutoRedTeam pipeline significantly improves model performance on safety benchmarks like ALERT, while maintaining strong general capabilities when appropriately mixed with helpfulness data. 

In Figure 1, we see that varying the data mix of helpfulness and AutoRedTeam samples leads to different results. To achieve maximum safety while maintaining comparable OpenLLM performance, we used a 70/30 split of AutoRedTeam and helpfulness data. Figure 2 shows a clearer comparison of this model against the *Llama-3.2-1B* Base and Instruct versions. 

From Figure 1, we can observe that increasing the proportion of helpfulness data and reducing the amount of redteaming instructions leads to better performance on helpfulness benchmarks, while still achieving strong results on safety benchmarks. This demonstrates that AutoRedTeam data is robust, diverse, and generalizes well.

It is also worth noting that IFEval performance could be further improved by adding specific IFEval-focused helpful instructions. However, the goal of these experiments was not to maximize helpfulness, but rather to evaluate the impact of incorporating redteam instructions during fine-tuning.

Our experiments validate that our fully synthetic AutoRedTeam approach provides valuable and diverse data for effective LLM red teaming, offering a promising path forward for policy alignment with the EU AI Act and broader safety standards.

## Discussion and Future Works

A particularly promising extension of our pipeline would be developing data creation capabilities for any given policy. While our current implementation focuses on EU AI Act compliance using LlamaGuard as an evaluator, the approach could be generalized to support arbitrary policy frameworks given appropriate filtering techniques.

The core benefit of our pipeline is that it generates data based on word ontology and grammar rules while lessening copyright concerns because the generated data is not based on others&apos; copyrighted text. This makes our approach valuable in scenarios where copyright infringement is a serious concern and cannot be tolerated.

This is an evolving blog, so check back in from time to time to get updates and welcome to our journey!


## Citation
```bibtex
@misc{autoredteam_2025,
  author       = {Huu Nguyen, Harsh Raj, Felix Friedrich, Ken Tsui, Victor May},
  title        = {AutoRedteam: Policy-Based Multimodal multilingual Data Generation},
  howpublished = {https://aurora-lm.github.io/posts/autoredteam},
  note         = {Accessed: 2025-04-24},
  year         = {2025}
}</content:encoded></item><item><title>MixtureVitae: A Permissive, High-Performance, Open-Access Pretraining Dataset</title><link>https://aurora-lm.github.io/posts/mixturevitae/</link><guid isPermaLink="true">https://aurora-lm.github.io/posts/mixturevitae/</guid><description>We introduce an open source pretraining dataset designed to lower legal copyright uncertainty while still delivering high-performance. Our dataset, called MixtureVitae, is intended to train high-quality LLMs and create transparency and open access for AI research.</description><pubDate>Sat, 12 Apr 2025 00:00:00 GMT</pubDate><content:encoded>**By: [Huu Nguyen](https://www.linkedin.com/in/huu-ai-machine-learning/), [Harsh Raj](https://harshraj172.github.io/), [Ken Tsui](https://www.linkedin.com/in/ken-tsui-06889b29/?originalSubdomain=uk), [Minh Chien Vu](https://scholar.google.com/citations?user=wcbZoCgAAAAJ&amp;hl=en), [Sonny Vu](https://scholar.google.com/citations?user=kFY-kEUAAAAJ&amp;hl=en), [Diganta Misra](https://digantamisra98.github.io/), [Victor May](https://mrcabbage972.github.io/), [Marianna Nezhurina](https://scholar.google.ru/citations?user=2KPv4VYAAAAJ&amp;hl=en), [Christoph Schuhmann](https://scholar.google.com/citations?user=EvrlaSAAAAAJ&amp;hl=en), [Robert Kaczmarczyk](https://scholar.google.com/citations?user=qj7YcjcAAAAJ&amp;hl=en), [Andrej Radonjic](https://x.com/0xdrej?lang=en), [Jenia Jitsev](https://www.linkedin.com/in/jenia-jitsev-11654427/?originalSubdomain=de) -- Apr 12, 2025**

&lt;div align=&quot;center&quot;&gt;

&lt;strong&gt;Huggingface:&lt;/strong&gt; &lt;a href=&quot;https://huggingface.co/datasets/ontocord/MixtureVitae&quot; style=&quot;color: #1f6feb;&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;MixtureVitae&lt;/strong&gt;&lt;/a&gt;

&lt;/div&gt;

Our blog is a way to share our journey with the wider community and is a living and organic document. We plan to update it continuously, alongside other posts on our site, as we further develop our data and models. In creating this dataset, we build on the work of industry giants such as Common Crawl, FineWeb, TxT360, Open License Corpus, Nemotron-CC, MAGAcorpus, Mint-PDF, and many others mentioned in the following sections. 

We curated permissive data through two primary avenues: first, by sourcing known public domain, out-of-copyright, or permissively licensed materials including text, video, and image data under licenses such as CC-BY-SA or open source software licenses; and second, by leveraging government websites that are more likely to fall under fair use, with our ethical and legal reasoning discussed in &lt;u&gt;Our Position For Using Governmental Works&lt;/u&gt; section.

In total, we collected approximately 300 billion text tokens of copyright-permissive data from various open sources, enriched with high-quality synthetic data generated through our own pipelines. We refer to this curated dataset and methodology as MixtureVitae. 

Looking forward, we will shortly translate to multiple languages to reach 1T tokens. The token count will then be increased again by tokenizing the multimodal image and sound data.

There is a common notion that permissive-only data does not yield stronger models. Recently, big tech companies have [suggested](https://www.theguardian.com/technology/2024/jan/08/ai-tools-chatgpt-copyrighted-material-openai) that stronger models aren&apos;t possible without copyrighted content. We challenge that assumption through careful curation, systematic ablations, and synthetic data generation.
## MixtureVitae
MixtureVitae is a *carefully curated collection* of diverse, high-quality sources, designed from the ground up to lessen risk of copyright issues, and improve reliability, and diversity across domains and modalities.

### How to use
The main portion of the dataset can be found at [HuggingFace](https://huggingface.co/datasets/ontocord/MixtureVitae).  However, there are subsets from the TXT360 and post-training datasets which we do not include in order to reduce duplications. You can find the links to these datasets below.

### What’s Inside the MixtureVitae?
We filtered, collated, reorganized and included data from various open corpora, and we created our own original data using synthetic data generation techniques. The corpus spans a rich variety of open datasets, curated datasets, and synthetic generations.

&lt;figure&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/aurora-lm/aurora-lm.github.io/main/assets/images/mixturevitae/pie-chart.png&quot; alt=&quot;Proportions of Open Web, Curated, and Synthetic Dataset in MixtureVitae&quot;&gt;
  &lt;figcaption&gt;Figure 1: The proportions of Open Web, Curated, and Synthetic Dataset in &lt;em&gt;MixtureVitae&lt;/em&gt;. Since both the Open Web and Curated datasets contain some synthetic data, we estimated the proportions excluding synthetic content (indicated as &quot;w/o synthetic&quot;) and grouped all synthetic portions into the Synthetic dataset category.&lt;/figcaption&gt;
&lt;/figure&gt;

#### Web Based Open Datasets
We filter a subset of the following web crawled datasets as described below. 

- [Nemotron-cc](https://arxiv.org/abs/2412.02595): A high-quality English dataset comprising 6.3 trillion tokens (4.4 trillion globally deduplicated original tokens and 1.9 trillion synthetically generated tokens), transformed from Common Crawl data. Nemotron-CC achieves better trade-offs between accuracy and data quantity by a combination of classifier ensembling, synthetic data rephrasing, and reduced reliance on heuristic filters. 
- [Cosmopedia v2](https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus/tree/main/cosmopedia-v2): An extensive synthetic dataset containing over 30 million files and 25 billion tokens, generated by Mixtral-8x7B-Instruct-v0.1. Cosmopedia includes a diverse range of topics and formats, including textbooks, blog posts, stories, and WikiHow articles, aiming to replicate the breadth of knowledge found in web datasets like RefinedWeb and RedPajama.
- [FineWeb-Edu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu): A refined subset of FineWeb, focused on educational and instructive content. This dataset cleans vast web text into usable, narrative-style educational resources.
- [Mint-1T](https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-40): an open-source Multimodal Interleaved dataset containing 1 trillion text tokens and 3.4 billion images. It introduces new data sources, including PDFs and ArXiv papers. From this dataset, we collected the subset of documents originating from &quot;.gov&quot; sources. 
- [Cultura-Y](https://huggingface.co/datasets/SEACrowd/culturay): A multilingual gem with contributions from over 70 languages, ensuring diverse cultural and linguistic representation.
- [TxT360](https://huggingface.co/datasets/LLM360/TxT360): A comprehensive dataset that globally deduplicates 99 CommonCrawl snapshots and 14 curated non-web data sources, including FreeLaw and PG-19. It includes approximately 5.7 trillion tokens of high-quality text data. We use the portion of the TxT-360 common crawl that has duplicate signals of 11 or above, meaning that these documents may be more important, since their content has been repeated.

#### Curated Datasets
- [Open License Corpus (OLC)](https://huggingface.co/datasets/kernelmachine/open-license-corpus): We were heavily inspired by the OLC corpus. This dataset supports domain-specific learning in areas such as law, science, and technology. All data is sourced from permissively licensed material (e.g., CC-BY). We include selected sources across eight domains, including legal texts such as case law (public domain) and the Pile of Law (CC BY-SA subset); arXiv abstracts and subsets of S2ORC (public domain and CC BY-SA); news sources such as public domain news and Wikinews (CC BY-SA); and encyclopedic entries from Wikipedia (CC BY-SA). We collected and further cleaned and reformatted a subset of this collection  and/or removed certain citations that are likely too difficult for models to memorize reliably.
- [WebSight](https://huggingface.co/datasets/HuggingFaceM4/WebSight): The Websights dataset comprises synthetically generated HTML/CSS code representing English websites, each accompanied by a corresponding screenshot. It serves as a valuable resource for tasks such as generating user interface code from visual inputs. We rephrased the WebSights dataset so that it is framed as instructions and responses. 
- [PG-19](https://huggingface.co/datasets/deepmind/pg19): We incorporated data from PG-19, comprising  28,752 books from Project Gutenberg published before 1919. This dataset serves as a valuable resource for training models to capture long-range dependencies in text. We use TXT360’s version of PG-19, and we further created shards of the books that are roughly 4096 tokens.
- [Freelaw](https://free.law/): Sourced from The Pile, this dataset comprises legal documents which improves the models knowledge about the law. 
- [StackV1](https://huggingface.co/datasets/bigcode/the-stack): We included several subsets of the stack v1 dataset, modifying them removing headers to remove duplicate text. We further created a subset by clustering certain portions based on minhash and concatenating similar documents into single examples. Specifically, we flattened the python-edu portion by combining files from the same repository into a single example. Specifically, we included the following programming languages: Antlr, AppleScript, Awk, Batchfile, Clojure, CMake, CoffeeScript, Common Lisp, C/C++, C#, Emacs Lisp, Fortran, Go, Java, JavaScript, Jupyter scripts and structured data, Makefile, Mathematica, Perl, Python, Rust, Scheme, Shell, SQL, Tcl, TypeScript, and Yacc. We also included multilingual code to enhance language diversity. In addition to code, we incorporated non-coding data formats such as Markdown, HTML, and JSON to provide structured documentation.
- [Euro-Pat](https://europat.net/): A parallel multilingual dataset comprising patent documents from the United States Patent and Trademark Office (USPTO) and the European Patent Organisation (EPO). It aligns patents from different countries within the same &quot;family,&quot; providing multilingual context. To further enrich this resource, we have generated synthetic images corresponding to the patent documents.
- [USPTO Data](https://data.uspto.gov/): Derived from TXT360 and The Pile and further cleaned for use in Aurora-m1, this dataset provides a comprehensive collection of U.S. patent documents. 
- [COCO Captions](https://cocodataset.org/#home): We recaptioned the coco-2017 dataset, commonly used to develop models for multimodal understanding.
- [OIG](https://huggingface.co/datasets/laion/OIG): We included certain subsets of the OIG dataset, rephrased with a LLM. Specifically, we incorporated the Unified SQL, Unified SKG, Abstract-Infil and Canadian Parliament subset of the OIG dataset. 
- [Europarl](https://www.statmt.org/europarl/): A parallel corpus sourced from TxT360 extracted from the proceedings of the European Parliament, covering 21 European languages.
- **10-K Filings**: A dataset comprises a collection of 10-K filings, which are comprehensive annual reports filed by publicly traded companies to the U.S. Securities and Exchange Commission (SEC). These documents provide detailed insights into a company&apos;s financial performance, risk factors, and management discussions.
- [Atticus](https://huggingface.co/datasets/aurora-m/aurora-m-dataset-part-1/tree/main/en): A comprehensive, expert-annotated dataset designed for research in legal contract review. It includes over 13,000 annotations across 510 commercial legal contracts, focusing on 41 categories of clauses deemed crucial in corporate transactions such as mergers and acquisitions.
- [Aozora Bunko](https://marketplace.sshopencloud.eu/dataset/ctbZd7): A digital library that hosts a vast collection of Japanese literary works that are in the public domain. The project aims to make classic Japanese literature accessible to the public by digitizing and providing texts in various formats.
- **Hackernews**: Sourced from OLC and TxT360, this dataset includes discussions and articles from the Hacker News platform.
- **StackExchange**: A dataset comprising of publicly available data from the Stack Exchange network, including users’  questions, answers, comments, and associated metadata. We included RedPajamav1’s and TxT360’s Stack Exchange dataset, encompassing a wide range of topics from the Stack Exchange network.
- [Openwebmath](https://huggingface.co/datasets/open-web-math/open-web-math): An open dataset of high-quality mathematical web text, containing 14.7 billion tokens extracted from mathematical webpages, intended for training language models in mathematical reasoning.
- [Wikibooks](https://en.wikibooks.org/wiki/Help:Database_download): A Wikimedia project that offers a collection of open-content textbooks and manuals across various subjects, including computing, science, humanities, and more. The content is collaboratively written and freely available under Creative Commons licenses.​
- **Pubmed Abstracts**: A dataset comprising abstracts from biomedical literature, sourced from PubMed. It includes millions of abstracts detailing research across various biomedical fields. This dataset was obtained from the Pile and TxT360 datasets.
- [Pubmed Central](https://catalog.data.gov/dataset/pubmed-central-pmc?utm_source=chatgpt.com): A free digital archive of full-text biomedical and life sciences journal literature. The dataset includes articles made available under licenses that permit text mining and other forms of reuse. It was extracted from the TxT360 dataset.
- [NIH ExPorter](https://reporter.nih.gov/exporter): A dataset includes administrative data on NIH-funded research projects. It provides detailed information on awarded grants, including abstracts, funding amounts, and project durations.​ This data was processed from the Pile dataset.
- [Elsevier-oa](https://huggingface.co/datasets/heegyu/elsevier-oa-cc-by): This corpus comprises over 40,000 open-access articles from Elsevier journals, available under the CC-BY license. It spans a wide range of scientific disciplines, such as medicine, biology, chemistry, physics, and engineering.
- [Clap Synthetic Captions](https://github.com/LAION-AI/audio-dataset): The CLAP dataset provides synthetic captions for audio clips, generated using advanced audio-language models. Includes captions for a large number of audio clips, with each clip accompanied by multiple synthetic captions.
- **arXiv Summaries**: Sourced from OLC, this dataset includes summaries of research papers from arXiv, facilitating research in scientific literature summarization.
- **Wikipedia**: We included portions of Wikipedia from TxT360’s subset. To limit non-educational value documents, we filtered out articles about currently living persons, such as sports stars, or shorter articles, including those about sporting events, inspired by Phi-3 and 4’s work. Additionally, we included clustered Wikipedia articles based on minhash clustering for languages: English (en), Italian (it), Polish (pl), Swedish (sv), and Dutch (nl).
- [Megawika](https://huggingface.co/datasets/hltcoe/megawika): A large-scale, multilingual, and cross-lingual dataset containing 30 million Wikipedia passages with their cleaned web citations in 50 languages. We used the original Wikipedia documents, select translations, and .gov web pages in this corpus, cleaning the content, aligning multilingual pairs, and appending relevant .gov webpages cited by Wikipedia. This results in a structured and contextualized knowledge base.
- [VALID](https://huggingface.co/datasets/ontocord/VALID): Developed by Ontocord.AI in collaboration with Grass and LAION, VALID includes approximately 720,000 Creative Commons-licensed YouTube videos. It combines: Video frames, Audio, Multilingual transcripts. This enables training on rich multimodal interactions, where models learn the interplay between vision, language, and sound. We use only the text portion of this dataset for now.
- [Youtube Commons](https://huggingface.co/datasets/PleIAs/YouTube-Commons): A collection of over 2 million YouTube videos released under CC-BY, providing 45 billion words of multilingual conversational transcripts. This hopefully will teach models how humans actually talk—across contexts, languages, and cultures. We collapsed transcripts for the same video into a single example so that the multilingual translations are presented together. We also performed text cleanup to remove some grammar and spelling issues. 


#### Synthetic Data

A large portion of our dataset is composed of synthetic data, intertwined and interleaved with organic data. For example, a large portion of our web datasets comes from [nemotron-cc](https://arxiv.org/abs/2412.02595) which is mostly synthetic data, and we have also curated synthetic translations from sources such as [Megawiki](https://huggingface.co/datasets/hltcoe/megawika). The pie chart in Figure 1 illustrates the distribution of data, where we estimated the proportions excluding synthetic content and grouped all synthetic portions into the Synthetic dataset category.

- **Multiple-Choice Question (MCQ) Generation**: We generate multiple-choice questions (MCQs) derived from rich knowledge sources such as GenericsKB, as well as abstract infill templates from the [OIG dataset](https://laion.ai/blog/oig-dataset/). These MCQs are intended to promote the ability to answer multiple choice questions.
- **Synthetic Stories**: We generated synthetic stories similar to [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories), based on datasets like [Atomic 2024](https://huggingface.co/datasets/ontocord/atomic_2024), and select portions of the [PG-19](https://huggingface.co/datasets/deepmind/pg19) dataset.
- **Mathematical Generation**:​ 
  - We create math textbooks for specific complex topics by combining mathematical instruction seed data with open-source mathematical web texts. The goal of this dataset is to teach the model about basic math such as arithmetic and algebra.
  - We also create synthetic math questions and answers to teach basic math as well.
- **Instruction Generation**: The instruction generation pipeline creates structured instructions, stories, and data analysis summaries, similar to Ultra-Magpie from [SmolLM](https://arxiv.org/abs/2502.02737v1), but without editing instructions. 

Additionally, We include widely-used open source and permissive post-training datasets from Hugging Face, reformatting them into a pretraining corpus by flattening their structures. While these are mostly intended for post-training, we incorporate these datasets at various stages of training to determine if they improve eval metrics. The current set of datasets incorporated are:​
- [nvidia/OpenMathInstruct-1](https://huggingface.co/datasets/nvidia/OpenMathInstruct-1): A math instruction tuning dataset with 1.8M problem-solution pairs generated using permissively licensed *Mixtral-8x7B* model. The problems are from [GSM8K](https://huggingface.co/datasets/openai/gsm8k) and [MATH]((https://github.com/hendrycks/math)) training subsets and the solutions are synthetically generated by allowing Mixtral model to use a mix of text reasoning and code blocks executed by Python interpreter.
- [ifeval-like-data](https://huggingface.co/datasets/argilla/ifeval-like-data): This dataset contains instruction-response pairs synthetically generated using *Qwen/Qwen2.5-72B-Instruct* following the style of [google/IFEval](https://huggingface.co/datasets/google/IFEval) dataset and verified for correctness with EleutherAI *lm-evaluation-harness*.
- [Bespoke-Stratos-17k](https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k): A dataset comprising 17,000 entries designed for model refinement and evaluation. It includes 5,000 coding-related samples sourced from the APPs and TACO datasets, 10,000 math-focused examples drawn from the AIME, MATH, and Olympiad subsets of the NuminaMATH dataset, and 1,000 science and puzzle problems from the STILL-2 dataset. The dataset is distilled from *DeepSeek-R1*.
- [SystemChat-2.0](https://huggingface.co/datasets/cognitivecomputations/SystemChat-2.0): A dataset designed for training conversational agents, including system-initiated dialogues across diverse scenarios. It is used to instill strong instruction-following abilities and adherence to system prompts.
- [Ultrafeedback_phi3_responses](https://huggingface.co/datasets/Seungyoun/ultrafeedback_phi3_responses): A subset of the [Ultrafeedback](https://huggingface.co/datasets/openbmb/UltraFeedback) dataset containing responses generated by the *Phi-3* model. Since the model we are training shares architectural similarities and data characteristics with Phi, this subset is chosen to align closely with our target model&apos;s behavior.
- [CaseHOLD_Phi4_Reasoning](https://huggingface.co/datasets/nguyenkhanh87/CaseHOLD_Phi4_Reasoning): A collection of *Phi-4* model responses on the [CaseHOLD](https://github.com/reglab/casehold) dataset, which focuses on legal case holdings. This dataset supports the development of models with advanced legal reasoning capabilities.
- [Magpie Collection](https://huggingface.co/Magpie-Align): A collection of a set of high-quality instruction datasets generated using the Magpie technique - a self-synthesis approach where aligned language models autonomously create diverse instruction-response pairs. It includes *Magpie-Gemma2-Pro-534K-v0.1*, containing 534K entries distilled from *Gemma-2-27B-Instruct* for general alignment and performance; *Magpie-Phi3-Pro-1M-v0.1*, with 1 million professional-grade samples distilled from *microsoft/Phi-3-medium-128k-instruct* for advanced instruction tuning; and *Magpie-Qwen2.5-Coder-Pro-300K-v0.1*, comprising 300K code-focused samples distilled from *Qwen2.5-Coder-32B-Instruct* for code generation tasks.
- [DeepScaler-QwQ_32b](https://huggingface.co/datasets/tttonyyy/DeepScaler-QwQ_32b/blob/main/distilled_s0_e20000_20250309005632_final.json): A specialized collection derived from the [DeepScalerR](https://github.com/agentica-project/rllm) dataset, comprising 20,000 samples. Each sample includes a question and its corresponding answer generated by the *Qwen/QwQ-32B* model, which is designed for advanced reasoning tasks such as mathematics and coding.
- [SYNTHETIC-1](https://huggingface.co/datasets/PrimeIntellect/SYNTHETIC-1): A reasoning-focused dataset distilled from *DeepSeek-R1* and annotated with a diverse set of verifiers, including LLM-based judges and symbolic mathematics verifiers. It includes a wide range of challenging tasks, such as competition-level math problems from [NuminaMath](https://huggingface.co/datasets/AI-MO/NuminaMath-CoT), coding problems from platforms like LeetCode, and curated datasets including [APPS](https://huggingface.co/datasets/codeparrot/apps), [CodeContests](https://huggingface.co/datasets/deepmind/code_contests), [Codeforces](https://huggingface.co/datasets/MatrixStudio/Codeforces-Python-Submissions), and [TACO](https://huggingface.co/datasets/BAAI/TACO). It also includes problems derived from real-world GitHub commits via the [CommitPack](https://huggingface.co/datasets/bigcode/commitpackft) dataset, as well as questions sourced from the [StackExchange dataset](https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences).
- [MetaMathQA-R1](https://huggingface.co/datasets/oumi-ai/MetaMathQA-R1): S text dataset designed to train LLMs with DeepSeek-R1 level reasoning. The prompts in this dataset are derived from the training sets of GSM8K and MATH, with responses generated directly by *DeepSeek-R1*.
[OpenR1-Math-220k](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k): A large-scale dataset for mathematical reasoning. It consists of 220k math problems with two to four reasoning traces generated by *DeepSeek R1* for problems from *NuminaMath 1.5*. The traces were verified using [Math Verify](https://github.com/huggingface/Math-Verify) for most samples and *Llama-3.3-70B-Instruct* as a judge for 12% of the samples.
- [OpenManus-RL](https://huggingface.co/datasets/CharlieDreemur/OpenManus-RL): Combines agent trajectories from [AgentInstruct](https://huggingface.co/datasets/THUDM/AgentInstruct), [Agent-FLAN](https://huggingface.co/datasets/internlm/Agent-FLAN), and [AgentTraj-L](https://huggingface.co/datasets/AgentGym/AgentTraj-L) (AgentGym) with key features including the [ReAct](https://react-lm.github.io/) prompting framework, structured training (separate format and reasoning learning), and anti-hallucination techniques (negative samples and environment grounding). Covers six domains: Operating Systems, Databases, Web, Knowledge Graphs, Household, and E-commerce.
  
Overall, the datasets is composed of roughly 20-30% synthetic data, with that percentage growing after we have completed translations and additional multimodal generation. We expect that a major portion of MixtureVitae will then be synthetic.

&lt;figure&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/aurora-lm/aurora-lm.github.io/main/assets/images/mixturevitae/flow.png&quot; alt=&quot;The process of filtering and compiling MixtureVitae&quot;&gt;
  &lt;figcaption&gt;Figure 2: Flowchart illustrating the process of filtering and compiling &lt;em&gt;MixtureVitae&lt;/em&gt;.&lt;/figcaption&gt;
&lt;/figure&gt;


### How Do We Filter Web Based Data?
#### Permissive filtering
We performed a &quot;pseudo-crawl&quot; of the &quot;open web dataset&quot; for .gov and similar websites. And we also searched for the keywords &quot;CC-BY-SA&quot; and similar keywords and heuristics. We also performed removal of documents with spam like keywords in English, and a high proportion of obscene or offensive words or child sexual abuse materials (CSAM) words in multiple languages. 

#### Dedup and heuristic filtering
Then we perform a global deduplication (across web based documents) of these sources using only a prefix based matching of the documents. This is because the source datasets have been already deduplicated, so we perform only light global deduplication. We then remove common ngrams that begins or ends documents as these are sometimes generic headers such as &quot;Home | Search&quot; etc. We find sentence duplicates and remove these if they have low proportions of stopwords, as we find these are likely uninformative text. We augment other sentence duplicates using a wordnet synonym substitution method. We also perform filtering of documents with high proportions of obscene words, adult content and content involving minors and sexual abuse.

#### Fasttext Filtering
One of the challenges of .gov web data  is that it can skew heavily toward regulatory or compliance-heavy content, leading to overrepresentation of certain tones or subject matter. To avoid a narrow, one-note model, we implemented genre and domain balancing — a process that boosts the proportion of diverse and underrepresented content types.

**Classification.** We implemented multi-layered FastText classifiers to give us more insight into our data and to enable us to sample diverse subsets.

- **Domain Classification**: We used data from [FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb) which categorizes text based on a standard set of taxonomies. This dataset was used to train a fastText classifier, which we then used to classify a large portion of our text. We use this model to classify a large portion of our text.
- **Classification Based on Pile Type**: We developed a fastText classifier to identify and categorize content from various sources present in the Pile dataset. The categories follows the sources in this dataset, including Pile-CC, PubMed Central, Books3, OpenWebText2, ArXiv, Github, FreeLaw, Stack Exchange, USPTO, Backgrounds, PubMed Abstracts, Gutenberg (PG-19), OpenSubtitles, Wikipedia (en), DM Mathematics, Ubuntu IRC, BookCorpus2, EuroParl, HackerNews, YoutubeSubtitles, PhilPapers, NIH ExPorter, and Enron Emails.  Note that we do not actually include most of these datasets from the Pile due to copyright concerns. Rather, we have classified our permissive text to help us decide on how to maintain diversity across domains. This classifier produces a lower quality signal, but we include this for researcher’s usage.
- **Register (Genres)**: We employ [TurkuNLP’s FastText classifier](https://turkunlp.org/register-annotation-docs/) to give us insight into the different registers (genres) in our dataset, ultimately allowing us to appropriately balancing types of contents. The classifier covers a wide array of web text styles, including Narrative, Informational Description, Opinion, Interactive Discussion, How-to/Instruction, Informational Persuasion, Lyrical, and Spoken. This classification draws from the TurkuNLP annotation framework, which identifies text purpose (e.g., to narrate, instruct, describe, persuade, or express opinion), to identify diverse genres sources like blogs, news reports, Q&amp;A forums, and instructional content.
- **Limitations.** At present, our classifiers are mainly English based and do not classify well non-English text.

**Ranking.** We also implemented multi-layered FastText classifiers to rank documents for quality data.
- **Mathematics Ranker**: To enhance mathematical reasoning, similar to DeepSeekMath, a fastText classifier is trained to recall mathematics text from the web. Our initial model uses 800,000 samples from [MiniPile](https://arxiv.org/abs/2304.08442), which represents a diverse subset of Pile as negative class, and 800,000 samples from [openwebmath](https://huggingface.co/datasets/open-web-math/open-web-math) as positive class. However, as we observe the initial classifier is not sensitive to latex mathematical symbols, and also inspired from DataComp-LM, we include instruction-format data drawing from [StackMathQA](https://huggingface.co/datasets/math-ai/StackMathQA), [OpenR1-Math-220k](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k), [MetaMathQA](https://huggingface.co/datasets/meta-math/MetaMathQA) and [Omni-MATH](https://huggingface.co/datasets/KbsdJames/Omni-MATH), and downsample openwebmath to maintain balance class.
- **Red Pajama Ranker**: We use [RedPajamas’s fastText classifier](https://arxiv.org/abs/2411.12372), which provides signals if a text is similar to content in websites referenced by Wikipedia.
- **Educational Value Ranker**: Inspired by Textbooks are All You Need, we use an open source [educational value classifier](https://huggingface.co/blog/kenhktsui/edu-value-classifier-cpu) to recall content with high educational value from the web. Its training data is constructed by employing Phi-3-mini-128k-instruct to annotate MiniPile with educational value.
- **Limitations.** At present, our rankers are mainly English based and do not rank well non-English text.
The ranking uses an ensemble method to assess the quality of web documents. This approach combines multiple scoring metrics to determine an overall quality score for each document. Notably, we rank web based documents including synthetic documents associated with web data such as MAGACorpus and nemotron-cc,  and do not rank documents from curated sources such as PG-19, The Stack v1, Wikipedia, Stack Exchange, etc., recognizing their established quality and relevance. We use an average score of High Quality Content Score, the Educational Score and Math score to create a Quality score. For high quality documents, our threshold is 0.3. Additionally for Math documents, we also consider documents that might fall below 0.3, but has a math score about 0.8.

### How Do We Filter Curated Data?
While curated data such as code data from the Stack v1 and Wikipedia are generally of medium to high quality, we perform further processing to improve the quality of the data. Since we multiple sources for the same data (e.g., Stackexchange from both TxT360 and RedPajama v1), we perform a deduplication within subsets. For all datasets, we filter documents that have  high proportions obscene or adult content and content involving minors and sexual abuse. We found that some documents include base64 encoded text which can confuse models, and thus we filtered out these documents.  For Wikipedia based documents, we filtered documents that are mainly about films, sporting events, and biographies of living persons. This was inspired by filtering techniques in [Phi-3](https://arxiv.org/abs/2404.14219).


### Our Position For Using Governmental Works
Our ethical and legal reasoning for using government web content—sourced from Common Crawl-related datasets—is as follows:
- **Public Purpose Alignment**: The content created by governments is normally meant to be shared with the public, and by using the data for training we are assisting this purpose. 
- **Purpose of Use**: From a legal perspective, the government works are being redistributed as part of an open source, no-fee dataset to be used to create models are less likely to be copyright violating. This purpose is clearly not to compete with the government’s own usage.
- **Effect on Potential Market**: We also think it is more likely to be fair use because the use of government website content is unlikely to have an effect on the potential market for the government’s website content because the government is unlikely to be making commercial use to compete with the content as the government is unlikely making commercial use. 
- **Nature of the Content**: The nature of the content is mostly public announcements, content of public interest, governmental functions or the like. Again, we believe there is strong public policy interest for fair use of this type of information. 
- **Amount Used**: While we use all or almost all of the content of the government website, the amount of usage is not determinative of fair-use or not fair-use. 
- **Federal vs. Non-Federal Works**: Lastly, US works created by the federal governments are generally not copyrightable. However, we recognize that this is not the case for other foreign governmental works, or non-federal works.

For these reasons, we believe using government website data is lower copyright risk. But, to minimize the risk further such as the potential inclusion of third party copyrighted works on government web-pages, we have included keyword filters such as &quot;All Rights Reserved&quot;, &quot;Copyright ©&quot;, etc. to further filter out government web pages that have these terms.

With that said, we do not and cannot guarantee that even with rigorous provenance tracking and standard filtering, that there is no copyright, so we recommend anyone who uses our datasets to consult their own attorney in their jurisdiction.

This is an evolving blog, so check back in from time to time to get updates and welcome to our journey!


## Citation
```bibtex
@misc{mixturevitae_2025,
  author       = {Huu Nguyen, Harsh Raj, Ken Tsui, Diganta Misra, Victor May, Vu Minh Chien, Andrej Radonjic, Jenia Jitsev},
  title        = {MixtureVitae: A Permissive, High-Performance, Open-Access Pretraining Dataset},
  howpublished = {https://aurora-lm.github.io/posts/mixturevitae},
  note         = {Accessed: 2025-04-12},
  year         = {2025}
}</content:encoded></item></channel></rss>