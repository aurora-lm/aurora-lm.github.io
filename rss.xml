<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>NovaSky</title><description>Next-generation Open Vision and AI @ Berkeley Sky Computing Lab</description><link>https://novasky-ai.github.io/</link><language>en-US</language><item><title>Unlocking the Potential of Reinforcement Learning in Improving Reasoning Models</title><link>https://novasky-ai.github.io/posts/sky-t1-7B/</link><guid isPermaLink="true">https://novasky-ai.github.io/posts/sky-t1-7B/</guid><description>We are excited to release Sky-T1-7B, a SOTA open-recipe 7B model on math reasoning tasks, trained with 4-step SFT-&gt;RL-&gt;SFT-&gt;RL from the Qwen2.5-Math-7B base model. We also release Sky-T1-mini, trained with simple Reinforcement Learning (RL) applied on top of the DeepSeek-R1-Distill-Qwen-7B model, achieving close to OpenAI o1-mini performance on popular math benchmarks. We conduct a series of ablation studies on SFT data scaling, RL scaling and model’s pass@k performance after SFT and RL. We observe that the Long CoT SFT in general enhances the model’s pass@k performance while RL lifts the model’s performance at lower generation budgets (i.e., pass@1), which sometimes come at a cost of the entropy of solutions.</description><pubDate>Thu, 13 Feb 2025 00:00:00 GMT</pubDate><content:encoded>**Figure 1:** Average accuracy of different models on four popular math reasoning tasks (i.e., AIME24, AMC23, MATH500, and OlympiadBench). Sky-T1-7B demonstrates SOTA performance among 7B models (left 4 bars) trained with &lt;10k distilled samples from strong teacher reasoning models and Sky-T1-mini reaches SOTA performance among all open-source 7B models, including those (5th-7th bars) trained with &gt;100k distilled samples from strong teacher reasoning models. *For [rstar-math](https://arxiv.org/abs/2501.04519) and [Qwen2.5-7B-SimpleRL](https://hkust-nlp.notion.site/simplerl-reason), since the model weights are not open source, we directly use their reported numbers. 

**By: [Shiyi Cao](https://shiyicao.com/), [Shu Liu](https://www.linkedin.com/in/slynl/), [Dacheng Li](https://dachengli1.github.io/), [Tyler Griggs](https://tyler-griggs.github.io/), [Kourosh Hakhamaneshi](https://www.linkedin.com/in/kourosh-hakhamaneshi-4816a58a), [Sumanth Hegde](https://sumanthrh.com/about/), [Eric Tang](https://erictang000.github.io/), [Shishir G. Patil](https://shishirpatil.github.io/), [Matei Zaharia](https://people.eecs.berkeley.edu/~matei/), [Joey Gonzalez](https://people.eecs.berkeley.edu/~jegonzal/), [Ion Stoica](https://people.eecs.berkeley.edu/~istoica/) -- Feb 13, 2025**

We are excited to release **Sky-T1-7B**, a SOTA open-recipe 7B model on math reasoning tasks, trained with 4-step SFT-&gt;RL-&gt;SFT-&gt;RL from the [Qwen2.5-Math-7B base model](https://huggingface.co/Qwen/Qwen2.5-Math-7B). We also release **Sky-T1-mini**, trained with simple Reinforcement Learning (RL) applied on top of the [DeepSeek-R1-Distill-Qwen-7B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B) model, achieving close to OpenAI o1-mini performance on popular math benchmarks.

In this blog post, we also introduce a series of RL-enhanced 7B models we trained using different recipes to develop a deeper understanding of the potential of reinforcement learning in enhancing model capabilities and its relationship with Supervised Fine-Tuning (SFT). In summary, in this blog post:
 - We show that RL can significantly improve the reasoning scores of a small model.
    - We demonstrate a recipe for training Sky-T1-7B with RL and SFT from the Qwen2.5-Math-7B base model using only 5k distilled data from a strong teacher model QwQ, **outperforming models trained with over 100k distilled data from a much stronger teacher model DeepSeek-R1 (e.g., OpenThinker-7B trained on 117k R1 responses)**. We open-source the training recipe and its artifact, Sky-T1-7B. Notably, Sky-T1-7B also reaches similar OlympiadBench performance as DeepSeek-R1-Distill-Qwen-7B, which is trained on 800K data distilled from DeepSeek-R1.
    - Second, we show that simple RL can further enhance the current SOTA 7B reasoning model DeepSeek-R1-Distill-Qwen-7B’s capability, resulting in a new SOTA open-weights 7B reasoning model Sky-T1-mini, with close to o1-mini performance. The training takes 36 hours using 8xH100s, which is around $870 according to Lambda Cloud Pricing.
 - We conduct a series of ablation studies on SFT data scaling, RL scaling and model’s pass@k performance after SFT and RL. **We observe that the Long CoT SFT in general enhances the model’s pass@k performance while RL lifts the model’s performance at lower generation budgets (i.e., pass@1), which sometimes come at a cost of the entropy of solutions.**


To foster community progress, we open-sourced all artifacts including the training code, training recipes, model weights, and evaluation scripts.
 - [**Github**](https://github.com/NovaSky-AI/SkyThought): Code for data generation, SFT, reinforcement learning training, and evaluation.
 - [**HuggingFace**](https://huggingface.co/collections/NovaSky-AI/sky-t1-7b-67ab281da8192c1ba3e5296c): The Huggingface collection for model checkpoints, final model weights and datasets used for **Sky-T1-7B** and **Sky-T1-mini**.


## Sky-T1-7B – Trained with 4-step SFT and RL

![img](https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/sky-t1-7b/7b.jpg)
**Table 1:** Benchmark performance of the intermediate models trained in the 4-step pipeline. The final model achieves accuracy improvement of +10.4% on AIME24, +33.2% on MATH500, +36.8% on AMC23, +32.1% on OlympiadBench, and +21.1% on average, compared to the base model.

### Step 1: SFT
We use the QwQ model to generate the distillation data since **the model was trained before the release of DeepSeek R1** and QwQ was the only open-weights long reasoning model at the time when we trained the model. For the data mixture, we use GPT-4o-mini to classify the difficulty of the prompts according to the AoPS standard and selected math problems of difficulty higher than Level 3, Olympiads higher than Level 8, and all AIME/AMC problems in the [NUMINA dataset](https://huggingface.co/datasets/AI-MO/NuminaMath-CoT). We then perform rejection sampling by only accepting the solutions that match the ground truth. In total, we curated [5K responses from QwQ](https://huggingface.co/datasets/NovaSky-AI/Sky-T1-7B-step1-sft-5k).
Finally, we use the 5K responses to perform SFT on the Qwen2.5-Math-7B using the [Sky-T1 system prompt](https://github.com/NovaSky-AI/SkyThought/blob/main/skythought/skythought_evals/models/model_configs.yaml). We trained the model for 3 epochs, using a learning rate of 1e-5, and a batch size of 96. After this stage, we get the [Sky-T1-7B-Step1](https://huggingface.co/NovaSky-AI/Sky-T1-7B-step1) model.

### Step 2: RL
Next, we apply the [PRIME](https://github.com/PRIME-RL/PRIME)’s algorithms to it. We use the [Eurus-2-RL-Data](https://huggingface.co/datasets/PRIME-RL/Eurus-2-RL-Data) for the RL training and run it for 127 steps with a batch size of 256 (~30K data). For each prompt, we generate 4 rollouts and adopt the prompt filtering optimization proposed in PRIME that filters out the problems for which all of the 4 rollouts are correct or wrong. After this stage, we get the [Sky-T1-7B-Step2](https://huggingface.co/NovaSky-AI/Sky-T1-7B-step2) model. This stage runs on 8xH100 for around 44 hours.

As suggested in [DeepSeek-V3 technical report’s](https://arxiv.org/pdf/2412.19437v1) sec 5.1, the model trained through SFT and RL can serve as a high-quality data generator. We therefore perform another round of distillation and rejection sampling on traces generated by Sky-T1-7B-Step2 and curated [5k SFT samples](https://huggingface.co/datasets/NovaSky-AI/Sky-T1-7B-step2-distill-5k) using the same data mixture in Step 1. We fine-tune the Qwen2.5-Math-7B with these 5k samples and obtained the Sky-T1-7B-Step2-5k-distill model, which surprisingly maintains similar or even better performance than Sky-T1-7B-Step2 across the 4 benchmarks, demonstrating extremely high data-efficiency compared to the model fine-tuned with 5k QwQ traces.

### Step 3: SFT Again
Together, with the 5K data distilled from Sky-T1-7B-Step2 in Step 2 and 5K data distilled from QwQ in Step 1, we perform another round of SFT on Qwen2.5-Math-7B base model. Similarly, we trained the model for 3 epochs, using a learning rate of 1e-5, and a batch size of 96. We then get the [Sky-T1-7B-step3](https://huggingface.co/NovaSky-AI/Sky-T1-7B-step3) model.

### Step 4: RL Again
In this stage, to speed up the RL training, we adopt the simple [RLOO](https://arxiv.org/abs/2402.14740) algorithm without using prompt filtering and process reward model. We use the numina_amc_aime and numina_olympiads subset of the [Eurus-2-RL-Data](https://huggingface.co/datasets/PRIME-RL/Eurus-2-RL-Data). We run the training for 59 steps with a batch size of 256 (~15K data). For each prompt, we generate 8 rollouts. We get [Sky-T1-7B](https://huggingface.co/NovaSky-AI/Sky-T1-7B) as the final model.

### Evaluation
For reproductivity, we perform all the evaluation using the [Qwen’s math evaluation suite](https://github.com/QwenLM/Qwen2.5-Math/blob/main/evaluation/sh/eval.sh). For AIME24 and AMC 23, since they only have 30 and 40 questions respectively, we evaluate their performance by sampling 8 times for each question with a temperature of 0.6 and a top-p sampling probability of 0.95 and then compute the [pass@1](https://arxiv.org/pdf/2107.03374) (the calculation script is also provided [here](https://github.com/NovaSky-AI/SkyThought/tree/main/scripts/qwen_eval_bon.py)). For MATH500 and OlympiadBench, we use greedy decoding.

### Results
We report the benchmark results for models after each stage as well as the intermediate distilled model in Table 1. We also plot the models’ pass@k curves to better understand how each SFT and RL stage impacts the model’s internal capability. For comparison, we conduct another ablation experiment which runs the RLOO directly on the Qwen2.5-Math-7B base model using the [STILL3](https://huggingface.co/datasets/RUC-AIBOX/STILL-3-Preview-RL-Data) dataset, with 4 rollouts for each prompt. We train for 104 steps and get the final model as Sky-T1-7B-Zero.

As shown in Figure 2, Long CoT SFT significantly improves the model’s overall pass@k performance in both AIME24 and AMC23. In AMC, the two-stage RL primarily boosts pass@1 accuracy while reducing the diversity of solutions for k = 4 to 32. In AIME, the step4 RL further enhances overall pass@k compared to the step1 SFT and step2 RL, though its impact is less pronounced compared to Sky-T1-7B-Zero.

![img](https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/sky-t1-7b/sft_rl_test_time.png)
**Figure 2:** Pass@K curves for models trained after each step for AIME24 and AMC23.

## Sky-T1-mini – Simple RL Boosts the Performance
Throughout our development of Sky-T1-7B (which was trained before the release of DeepSeek R1’s release), we found that simple RL algorithms without a Process Reward Model (PRM) work well to enhance the model’s performance. Therefore, we also apply the simple RLOO algorithm with only the verifier reward on [DeepSeek-R1-Distill-Qwen-7B]((https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)), the current SOTA open-source 7B reasoning model, using the [STILL3](https://huggingface.co/datasets/RUC-AIBOX/STILL-3-Preview-RL-Data) dataset and the numina_amc_aime and numina_olympiads subset in the [Eurus-2-RL-Data](https://huggingface.co/datasets/PRIME-RL/Eurus-2-RL-Data) dataset. We run it for 119 steps (~28 hours) with a batch size of 256 (~30k) on 8xH100, with a cutoff length of 8k and then run it for 29 steps (~8.7 hours) with a cutoff length of 16k. The final model, Sky-T1-mini, approaches o1-mini performance across the four math benchmarks, as reported in Figure 3. **While we only trained the model for a short period of time with contexts cutoff (we also didn&apos;t carefully choose the algorithms and data mixtures), the accuracy improvement is still impressive: +4% on AIME, +5.6% on OlympiadBench and +2% on average, demonstrating the potential of RL in further enhancing model&apos;s performance beyond distillation.**

## Complete Results
![img](https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/sky-t1-7b/performance_stats_avg.png)
**Figure 3:** Accuracy of Sky-T1-7B and Sky-T1-mini on AIME23, AMC23, MATH500, and OlympiadBench, compared with other 7B models.


## Other Observations
![img](https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/sky-t1-7b/sft_scale_rl.png)
**Figure 4:** Benchmark performance of models trained with different sizes of SFT data and those further enhanced with RL.

![img](https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/sky-t1-7b/passk_sft_scale.png)
**Figure 5:** Pass@K curves for models trained with different sizes of SFT data and those further enhanced with RL for AMC23.

To evaluate the impact of scaling Long CoT SFT data sizes, we scale QwQ traces from 30k to 60k to 120k. We report the benchmark performance and AMC pass@k curves for models trained with SFT and those further enhanced with RL in Figure 4 and Figure 5 respectively. The RL training here adopts the simple RLOO algorithm, using the [STILL3](https://huggingface.co/datasets/RUC-AIBOX/STILL-3-Preview-RL-Data) dataset, with 4 rollouts per prompt.

From the benchmark performance plot as shown in Figure 4, while SFT enables scaling from 30k to 60k, its effectiveness plateaus beyond this point. In contrast, models trained further with RL continue to benefit from increased data, demonstrating further improvements when scaling up to 120k. This highlights the importance of RL in effectively leveraging additional SFT training data.

A similar pattern emerges in pass@k evaluations as shown in Figure 5. When data scales from 30k to 60k and 120k, both SFT and RL show improvement in pass@k accuracy, with RL consistently achieving better test-time scaling across data sizes than SFT. Compared to scaling from 30k to 60k, the improvements from 60k to 120k are less pronounced for both SFT and RL.

This figure also shows that RL primarily enhances efficiency by improving its pass@k accuracy at lower generation budgets (i.e., for small k), effectively lifting performance without requiring excessive sampling. However, this may come at a trade-off of entropy of solutions – less gains with extensive parallel sampling.


## Conclusion
In this blog post, we show that RL can further enhance the model&apos;s capability on either lightly- or heavily-distilled models. We further conduct the pass@k experiments to demonstrate how SFT and RL will affect the model&apos;s pass@k performance. We observe that the Long CoT SFT in general enhances the model’s pass@k performance while RL lifts the model’s performance at lower generation budgets (i.e., pass@1), which sometimes come at a cost of the entropy of solutions.


## Acknowledgement
This work is done at [Berkeley Sky Computing Lab](https://sky.cs.berkeley.edu/) with generous compute support from [Anyscale](https://www.anyscale.com/), [Databricks](https://www.databricks.com/), and [Lambda Labs](https://lambdalabs.com/service/gpu-cloud?srsltid=AfmBOop5FnmEFTkavVtdZDsLWvHWNg6peXtat-OXJ9MW5GMNsk756PE5).


## Citation
```bibtex
@misc{sky-t1-7b,
  author       = {NovaSky Team},
  title        = {Unlocking the Potential of Reinforcement Learning in Improving Reasoning Models},
  howpublished = {https://novasky-ai.github.io/posts/sky-t1-7b},
  note         = {Accessed: 2025-02-13},
  year         = {2025}
}</content:encoded></item><item><title>Think Less, Achieve More: Cut Reasoning Costs by 50% Without Sacrificing Accuracy</title><link>https://novasky-ai.github.io/posts/reduce-overthinking/</link><guid isPermaLink="true">https://novasky-ai.github.io/posts/reduce-overthinking/</guid><description>We introduce Sky-T1-32B-Flash, our reasoning model that cuts generation length by up to 50% while maintaining accuracy.</description><pubDate>Thu, 23 Jan 2025 00:00:00 GMT</pubDate><content:encoded>**By: [Tyler Griggs](https://tyler-griggs.github.io/), [Shiyi Cao](https://shiyicao.com/), [Dacheng Li](https://dachengli1.github.io/), [Shu Liu](https://www.linkedin.com/in/slynl/), [Shishir G. Patil](https://shishirpatil.github.io/), [Matei Zaharia](https://people.eecs.berkeley.edu/~matei/), [Joey Gonzalez](https://people.eecs.berkeley.edu/~jegonzal/), [Ion Stoica](https://people.eecs.berkeley.edu/~istoica/) -- Jan 23, 2025**

We are excited to introduce **Sky-T1-32B-Flash**, our updated reasoning language model that significantly reduces overthinking, **slashing inference costs on challenging questions by up to 57%**. 

This enhancement decreases generation length while preserving accuracy across domains such as mathematics, coding, science, and general knowledge, and **requires only $275 for the complete training recipe** using 8xH100s according to Lambda Cloud pricing. 

To foster transparency and collaboration, we have open-sourced the full pipeline—from data generation and pre-processing to preference optimization and evaluation scripts—and openly provide the model weights and data.
 - [**Github**](https://github.com/NovaSky-AI/SkyThought): Code for data generation, response rewriting, preference optimization, and evaluations.
 - [**Dataset**](https://huggingface.co/datasets/NovaSky-AI/Sky-T1_preference_data_10k): 10K preference pairs 
 - [**HuggingFace**](https://huggingface.co/NovaSky-AI/Sky-T1-32B-Flash): Sky-T1-32B-Flash model weights

![img](https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/reduce-overthinking/headline-plot.png)
**Figure 1:** Our new model significantly reduces generated token lengths while maintaining strong performance on challenging benchmarks.

## What is overthinking?
Overthinking refers to reasoning models’ tendency to produce unnecessarily long responses, often with redundant or excessive reasoning steps. In line with the findings of [recent work](https://arxiv.org/abs/2412.21187), we observe that reasoning models, including NovaSky’s recently released [Sky-T1-32B-Preview](https://novasky-ai.github.io/posts/sky-t1/), [QwQ](https://huggingface.co/Qwen/QwQ-32B-Preview), and [R1](https://huggingface.co/deepseek-ai/DeepSeek-R1), often produce reasoning sequences with multiple proposed solutions each followed by double-checking transitions such as “Alternatively,” “But wait,” or “Let me reconsider”. While double-checking can detect errors and refine solutions, it often results in repetitive validations of simple or already-validated steps, creating inefficiency. For example, in response to the question “What is 1+1,?” Sky-T1-32B-Preview produces over 1000 tokens and more than 10 of these double-checking transitions.

## Benefits of reducing overthinking
Reducing overthinking improves efficiency and scalability by reducing redundant or unnecessary token generation. This improvement not only greatly reduces inference costs for reasoning models, but also offers multiple downstream benefits. First, the accelerated response delivery provides a much higher-quality user experience. Further, with more efficient reasoning, test-time generation methods such as Best-of-N, Majority Vote, or Monte Carlo Tree Search can yield higher accuracy within fixed computational budgets. It also streamlines data generation in self-training pipelines, which are often bottlenecked by large-scale data generation runs.

## How to reduce overthinking?
Our approach to reduce overthinking builds on the self-training recipe proposed in [recent work](https://arxiv.org/abs/2412.21187) with important enhancements to improve accuracy in challenging benchmarks across multiple domains. A challenge of reducing overthinking is to prevent the model from *underthinking*, where the model proposes a final solution without sufficiently validating it. This challenge is especially highlighted in the most challenging benchmarks where extensive double-checking and backtracking are required. Ideally, the model learns to adjust the depth of its reasoning based on the complexity of the question.

Our training process involves three primary stages: data generation, response rewriting, and preference optimization. 

![img](https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/reduce-overthinking/recipe.png)

### Stage 1) Data Generation
We used Sky-T1-32B-Preview to generate responses to the 12K questions in the [PRM800K](https://huggingface.co/datasets/tasksource/PRM800K) dataset. For each question, we used a temperature of 1.0 and generated 8 responses to create a diversity of response lengths. We then formed preference pairs to contrast “verbose” vs. “concise” solutions. Specifically, from the generated responses, we picked the shortest correct response as the positive example and the longest correct response as the negative example. We discarded the rest of the generated responses, and discard any questions that did not produce at least two correct responses. We hypothesize that preference optimization over such pairs can encourage the model to reduce overthinking. 

Preference optimization with these pairs reduced generation lengths and mostly maintained accuracy on several benchmarks ([MATH500](https://huggingface.co/datasets/di-zhang-fdu/MATH500), [GPQA](https://huggingface.co/datasets/Idavidrein/gpqa), [MMLU](https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro)), however, we observed accuracy degradation on challenging problems in coding ([LiveCodeBench](https://livecodebench.github.io/)-Medium and -Hard) and the most challenging math suites, [AIME24](https://huggingface.co/datasets/tasksource/PRM800K) and  MATH500 Level 5. These results suggest that the model was underthinking in cases requiring more complex reasoning. To address this, we used the initial dataset of 8 responses per question to add 1K preference pairs to our training data, where the negative example is the shortest *incorrect* response and the positive example is the shortest correct response that is longer than the negative example, ensuring the model retained its ability to engage in deeper reasoning when necessary. This new data mix brought the model back up to par with Sky-T1-32B-Preview on the most challenging math benchmarks.

&gt; **Recipe Enhancement #1:** Incorporate {short incorrect response, long correct response} into the preference pair dataset to encourage complex thinking for challenging problems.

Interestingly, preference optimization with this math-only dataset reduced generation length by &gt;25% in the *coding* domain while maintaining accuracy on LCB-Easy. However, we observed a drop in accuracy in the more challenging benchmarks LCB-Medium and -Hard, so we added 500 more preference pairs generated by Sky-T1-32B-Preview on the [TACO](https://huggingface.co/datasets/BAAI/TACO/tree/main) dataset. We again generated 8 responses with a temperature of 1.0 and created preference pairs with the shortest and longest correct responses, which elevated coding performance back to the level of Sky-T1-32B-Preview.

&gt; **Recipe Enhancement #2:** Incorporating a small number of coding preference pairs simultaneously boosts coding accuracy and further reduces coding generation lengths. 

Stage 1 required ~8 hours on 8xH100-80GB for a total of ~$190 according to Lambda Cloud pricing.

### Stage 2) Response Rewriting
We refined positive samples by removing unnecessary sub-solutions. The model’s reasoning sequences often include multiple proposed solutions each followed by double-checking transitions such as “Alternatively…,” “But wait…,” or “Let me reconsider…”. For easier questions, these transitions rarely lead to an altered answer but can extend the response length significantly. Using techniques inspired by [recent work](https://arxiv.org/abs/2412.21187), we use [Llama3.3-70B](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) to separate the solutions within a response then rewrite the response to include only the first correct sub-solution (FCS) and one additional sub-solution (+1). This pruning approach removes most of the unnecessary sub-solutions, reducing the sequence length of positive samples, but includes a single additional sub-solution to maintain the model’s long chain-of-thought reasoning structure. 

Following [prior work](https://arxiv.org/abs/2412.21187), we also explored rewriting the response to include up to the first correct solution (FCS) or up to the second correct solution (FCS+Reflection), but found our FCS+1 approach to achieve lowest generation lengths while maintaining accuracy. For coding samples, we did not perform response rewriting. We could not apply our FCS+1 approach to coding because responses almost never propose multiple complete code blocks as solutions, though we believe there is opportunity to remove significant redundancy in coding responses. We have open-sourced the response rewriting pipeline to enable researchers to easily explore alternative methods. 

&gt; **Recipe Enhancement #3:** Rewriting positive preference math examples to maintain only the first correct solution and one additional solution (FCS+1) maintains accuracy (unlike FCS) and produces shorter generation lengths (relative to FCS+R). 

Stage 2 required ~1 hour on 8xH100-80GB for a total of ~$25 according to Lambda Cloud pricing.

### Stage 3) Preference Optimization
We employed [SimPO](https://arxiv.org/abs/2405.14734) for preference optimization. SimPO is closely related to [DPO](https://arxiv.org/abs/2305.18290), but incorporates a length-normalized implicit reward into the optimization approach, which leads to shorter sequence lengths relative to DPO. Further, SimPO eliminates the need for the reference model required by DPO, making preference optimization less compute-intensive and therefore cheaper. As an alternative to preference optimization, we also explored using only SFT with the shortest responses, but found sequence lengths were only marginally reduced (&lt;5%). In the [ablation results](#ablations), we include ablations for DPO using the same preference pairs as described in Stage (2) and for SFT using the shortest responses.

We start with Sky-T1-32B-Preview as our base model and train with SimPO for 1 epoch and a batch size of 96. We found SimPO results to be sensitive to hyperparameter settings and performed limited exploration within the following space: learning rate = {1e-7, 5e-7, 1e-6}, gamma = {0.3, 0.5, 1.0}, beta = {2.0, 2.5}. We achieved the best performance with a learning rate of 5e-7, gamma of 0.3, and beta of 2.0.  We use [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory) to perform training.

Stage 3 required ~2.5 hours on 8xH100-80GB for a total of ~$60 according to Lambda Cloud pricing.


## Results
**Sky-T1-32B-Flash** maintains **Sky-T1-32B-Preview**’s accuracy across the suite of challenging benchmarks, and consistently reduces generation lengths by over 30%. Even on the most challengine problems, from AIME24 and LCB-Hard, Sky-T1-32B reduces sequence lengths by 37% and 57%, respectively. 

![img](https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/reduce-overthinking/results-table.png)

### Ablations
We report ablation results for alternative methods and recipes we explored. **LS** refers to using {Negative: **L**ongest correct example, Positive: **S**hortest correct example} preference pairs. **SILC** refers to using {Negative: **S**hort **I**ncorrect example, Positive: **L**ong **C**orrect example}.

![img](https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/reduce-overthinking/ablations-table.png)

## Acknowledgement
This work is done at [Berkeley Sky Computing Lab](https://sky.cs.berkeley.edu/) with generous compute support from [Anyscale](https://www.anyscale.com/), [Lambda Labs](https://lambdalabs.com/service/gpu-cloud?srsltid=AfmBOop5FnmEFTkavVtdZDsLWvHWNg6peXtat-OXJ9MW5GMNsk756PE5), and [Databricks](https://www.databricks.com/).


## Citation
```bibtex
@misc{reduce_overthinking_2025,
  author       = {NovaSky Team},
  title        = {Think Less, Achieve More: Cut Reasoning Costs by 50% Without Sacrificing Accuracy},
  howpublished = {https://novasky-ai.github.io/posts/reduce-overthinking},
  note         = {Accessed: 2025-01-23},
  year         = {2025}
}</content:encoded></item><item><title>Sky-T1: Train your own O1 preview model within $450</title><link>https://novasky-ai.github.io/posts/sky-t1/</link><guid isPermaLink="true">https://novasky-ai.github.io/posts/sky-t1/</guid><description>We introduce Sky-T1-32B-Preview, our reasoning model that performs on par with o1-preview on popular reasoning and coding benchmarks.</description><pubDate>Fri, 10 Jan 2025 00:00:00 GMT</pubDate><content:encoded>We introduce Sky-T1-32B-Preview, our reasoning model that performs on par with o1-preview on popular reasoning and coding benchmarks. **Remarkably, Sky-T1-32B-Preview was trained for less than $450, demonstrating that it is possible to replicate high-level reasoning capabilities affordably and efficiently.** All [code](https://github.com/NovaSky-AI/SkyThought) is open-source. 

![img](https://raw.githubusercontent.com/NovaSky-AI/novasky-ai.github.io/main/assets/images/sky-t1/Sky-T1-pipeline.jpg)

## Overview
Models such as o1 and Gemini 2.0 flash thinking that excel in reasoning have shown to solve complex tasks by producing a long internal chain of thought, among other advancements. However, the technical details and model weights are un-accessible, presenting a barrier to the participation of the academic and open-source communities.

In response, a few notable efforts have emerged to train open-weight reasoning models in the math domain, such as [Still-2](https://arxiv.org/abs/2412.09413) and [Journey](https://arxiv.org/abs/2411.16489). Concurrently, we, the NovaSky team at UC Berkeley, have been exploring various techniques to evolve the reasoning capabilities of base and instruct-tuned models. In this work, we achieve competitive reasoning performance not just in math, but also in coding in the same model.

### Fully Open-source: Driving Progress Together
To ensure our work benefits the broader community, we are fully committed to open-source collaboration. We open-source all details (i.e., data, codes, model weights) to enable the community to replicate and improve on our results *easily*:
 - [**Infrastructure**](https://github.com/NovaSky-AI/SkyThought): to build the data, train, and evaluate the model in a single repository.
 - [**Data**](https://github.com/NovaSky-AI/SkyThought): 17K data used to train Sky-T1-32B-Preview.
 - [**Technical details**](https://novasky-ai.github.io/posts/sky-t1): Our technical [report](https://novasky-ai.github.io/posts/sky-t1/) with a [wandb log](https://api.wandb.ai/links/sky-posttraining-uc-berkeley/wjg3sybl).
 - [**Model weights**](https://huggingface.co/NovaSky-AI): Our 32B model weight.

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th style=&quot;background-color: #bfbfbf;&quot;&gt;&lt;div align=&quot;center&quot;&gt;Sky-T1-32B-Preview&lt;/div&gt;&lt;/th&gt;
      &lt;th&gt;&lt;div align=&quot;center&quot;&gt;STILL-2&lt;/div&gt;&lt;/th&gt;
      &lt;th&gt;&lt;div align=&quot;center&quot;&gt;Journey&lt;/div&gt;&lt;/th&gt;
      &lt;th&gt;&lt;div align=&quot;center&quot;&gt;QwQ&lt;/div&gt;&lt;/th&gt;
      &lt;th&gt;&lt;div align=&quot;center&quot;&gt;o1&lt;/div&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Data&lt;/td&gt;
      &lt;td style=&quot;background-color: #f2f2f2;&quot;&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Code&lt;/td&gt;
      &lt;td style=&quot;background-color: #f2f2f2;&quot;&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Report&lt;/td&gt;
      &lt;td style=&quot;background-color: #f2f2f2;&quot;&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Math Domain&lt;/td&gt;
      &lt;td style=&quot;background-color: #f2f2f2;&quot;&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Coding Domain&lt;/td&gt;
      &lt;td style=&quot;background-color: #f2f2f2;&quot;&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Weights&lt;/td&gt;
      &lt;td style=&quot;background-color: #f2f2f2;&quot;&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;✅&lt;/div&gt;&lt;/td&gt;
      &lt;td&gt;&lt;div align=&quot;center&quot;&gt;❌&lt;/div&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

By sharing all these resources, we aim to empower the academic and open-source communities to build on our work, explore new possibilities, and push the boundaries of reasoning model development.

## Recipes
### Data Curation Process
To generate our training data we use QwQ-32B-Preview, an open-source model with reasoning capabilities comparable to o1-preview. We curate the data mixture  (see later section) to cover diverse domains that require reasoning, and a reject sampling procedure to improve the data quality. We then rewrite QwQ traces with GPT-4o-mini into a well-formatted version, inspired by [Still-2](https://arxiv.org/abs/2412.09413), to improve data quality and ease parsing. We particularly find the ease of parsing advantageous for reasoning models - they are trained to respond in a particular format, where results are often hard to parse. For instance, on the APPs dataset, without reformatting, we can only assume that the code is written in the last code block, where QwQ only achieves ~25% accuracy. However, sometimes code can be written in the middle, where after reformatting, the accuracy is boosted to higher than 90%.

**Rejection Sampling:** We discard QwQ samples if they are incorrect according to the solutions provided in datasets. For Math problems, we do exact matching with the ground truth solutions. For coding problems, we execute the unit tests provided in datasets. Our final data contains 5k coding data from APPs and TACO, and 10k math data from AIME, MATH, and Olympiads subsets of the NuminaMATH dataset. In addition, we maintain 1k science and puzzle data from STILL-2.

### Training
We use our training data to fine tune Qwen2.5-32B-Instruct, an open source model without reasoning capabilities. The model is trained with 3 epochs, learning rate 1e-5 and batch size 96. The model training finishes in 19 hours on 8 H100 with DeepSpeed Zero-3 offload (~ $450 according to Lambda Cloud pricing). We use [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory) to perform training.

### Evaluation and Results
&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th style=&quot;background-color: #bfbfbf;&quot;&gt;Sky-T1-32B-Preview&lt;/th&gt;
      &lt;th&gt;Qwen-2.5-32B-Instruct&lt;/th&gt;
      &lt;th&gt;QwQ&lt;/th&gt;
      &lt;th&gt;o1-preview&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Math500&lt;/td&gt;
      &lt;td style=&quot;background-color: #F2F2F2;&quot;&gt;82.4&lt;/td&gt;
      &lt;td&gt;76.2&lt;/td&gt;
      &lt;td&gt;85.4&lt;/td&gt;
      &lt;td&gt;81.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;AIME2024&lt;/td&gt;
      &lt;td style=&quot;background-color: #F2F2F2;&quot;&gt;43.3&lt;/td&gt;
      &lt;td&gt;16.7&lt;/td&gt;
      &lt;td&gt;50.0&lt;/td&gt;
      &lt;td&gt;40.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LiveCodeBench-Easy&lt;/td&gt;
      &lt;td style=&quot;background-color: #F2F2F2;&quot;&gt;86.3&lt;/td&gt;
      &lt;td&gt;84.6&lt;/td&gt;
      &lt;td&gt;90.7&lt;/td&gt;
      &lt;td&gt;92.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LiveCodeBench-Medium&lt;/td&gt;
      &lt;td style=&quot;background-color: #F2F2F2;&quot;&gt;56.8&lt;/td&gt;
      &lt;td&gt;40.8&lt;/td&gt;
      &lt;td&gt;56.3&lt;/td&gt;
      &lt;td&gt;54.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LiveCodeBench-Hard&lt;/td&gt;
      &lt;td style=&quot;background-color: #F2F2F2;&quot;&gt;17.9&lt;/td&gt;
      &lt;td&gt;9.8&lt;/td&gt;
      &lt;td&gt;17.1&lt;/td&gt;
      &lt;td&gt;16.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GPQA-Diamond&lt;/td&gt;
      &lt;td style=&quot;background-color: #F2F2F2;&quot;&gt;56.8&lt;/td&gt;
      &lt;td&gt;45.5&lt;/td&gt;
      &lt;td&gt;52.5&lt;/td&gt;
      &lt;td&gt;75.2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;


## Other findings
**Model size matters.** We initially experimented with training on smaller models (7B and 14B) but observed only modest improvements. For example, training Qwen2.5-14B-Coder-Instruct on the APPs dataset resulted in a slight performance increase on LiveCodeBench from 42.6% to 46.3%. However, upon manually inspecting outputs from smaller models (those smaller than 32B), we found that they frequently generated repetitive content, limiting their effectiveness.


**Data mixture matters.** We initially trained a 32B model using 3–4K math problems from the Numina dataset (provided by STILL-2), achieving a significant improvement in AIME24 accuracy from 16.7% to 43.3%. However, when we incorporated coding data generated from the APPs dataset into the training process, AIME24 accuracy dropped to 36.7%. We hypothesize that this decline is due to the distinct reasoning approaches required for math and coding tasks.

Reasoning in coding often involves additional logical steps, such as simulating test inputs or internally executing generated code, whereas reasoning for math problems tends to be more direct and structured. To address these differences, we enriched the training data with challenging math problems from the NuminaMath dataset and complex coding tasks from the TACO dataset. This balanced data mixture enabled the model to excel in both domains, restoring 43.3% accuracy on AIME24 while also improving its coding capabilities.

## Future work
Sky-T1-32B-Preview marks the start of our journey to develop open-sourced models with advanced reasoning capabilities. Moving forward, we will focus on developing more efficient models that maintain strong reasoning performance and exploring advanced techniques that further enhance the models’ efficiency and accuracy at test time. Stay tuned as we make progress on these exciting initiatives.


## Acknowledgement
This work is done at [Berkeley Sky Computing Lab](https://sky.cs.berkeley.edu/), with the amazing compute support from [Lambda Labs](https://lambdalabs.com/service/gpu-cloud?srsltid=AfmBOop5FnmEFTkavVtdZDsLWvHWNg6peXtat-OXJ9MW5GMNsk756PE5) and [Anyscale](https://www.anyscale.com/). We would like to express our gratitude for the valuable academic feedback and support from the [Still-2 Team](https://arxiv.org/pdf/2412.09413), and Junyang Lin from the [Qwen Team](https://qwenlm.github.io/).

## Citation
```bibtex
@misc{sky_t1_2025,
  author       = {NovaSky Team},
  title        = {Sky-T1: Train your own O1 preview model within $450},
  howpublished = {https://novasky-ai.github.io/posts/sky-t1},
  note         = {Accessed: 2025-01-09},
  year         = {2025}
}</content:encoded></item></channel></rss>